\documentclass[a4paper,12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc} % aggiunto per caratteri accentati
\usepackage{geometry}
\geometry{margin=3cm}
\usepackage{tikz}
\usepackage{listings}
\usepackage{xcolor} % per colori opzionali
\usepackage{graphicx}
\usepackage{float}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{array} 
\usepackage{booktabs}  

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{blue!10},   % sfondo blu chiaro
    frame=single,                      % bordo attorno al codice
    language=Python,                   % evidenziazione Python
    basicstyle=\ttfamily\small,       % font monospace
    keywordstyle=\color{blue},        % parole chiave blu
    commentstyle=\color{green!60!black}, % commenti in verde
    stringstyle=\color{orange},       % stringhe arancioni
    showstringspaces=false            % non mostra spazi nelle stringhe
}

\lstset{style=mystyle}


\begin{document}

% Pagina iniziale
\begin{titlepage}
    \centering
    \vspace*{5cm}
    {\Huge\bfseries Sviluppo e ciclo vitale di software di intelligenza artificiale\par}
    \vfill
    \begin{flushright}
        \large Angelo Vaccaro
    \end{flushright}
\end{titlepage}

\tableofcontents
\newpage

\section{Pre Requisiti per il corso}
\begin{itemize}
     \item Conoscenza della bash del codice Unix/Linux
     \item Conoscenza di base di Python
        \begin{itemize}
            \item operatori del linguaggio di programmazione (if/else, for loop, while, etc)
            \item differenza tra compilare ed eseguire uno script
        \end{itemize}
    \item Basi di Machine Learning e Deep Learning
\end{itemize}

\section{Obbiettivi del corso}
Il corso prevede quello di affrontare un problema con Machine learning e deep learning. Cosa devo fare?
Devo capire come muovermi e cos'è il mio problema analizzando i dati a disposizione.
Una volta che ho risolto il mio problema devo farlo andare in sviluppo, per farlo funzionare su più macchine.
Dobbiamo quindi chiederci come facciamo a sviluppare in modo sensato il software?
Dopo averlo sviluppato, come facciamo a farlo funzionare in produzione?

\section{Programma del corso}
\begin{itemize}
    \item Day 1
    \begin{itemize}
        \item Vesrion Control System (VCS)
        \begin{itemize}
            \item Git
            \item GitHub
        \end{itemize}
        \item Differenza tra DEVOps ed MLOps che sono i 2 processi di produzione del codice, uno nell'ambito di produzione normale mentre l'altro nell'ambito di machine learning e deep learning.
    \end{itemize}
        \item Day 2
    \begin{itemize}
        \item Data science, necessaria per investigare il problema e capire come risolverlo.
        \item Capiremo come addestrare un modello from scratch, ovvero da zero, senza utilizzare librerie preconfezionate.
        \item Piccolo accenno sul discorso degli iperparametri, che sono i parametri che dobbiamo settare per far funzionare il nostro modello.
    \end{itemize}
        \item Day 3
        \begin{itemize}
            \item Parleremo di Docker, che è un software che ci permette di creare dei container, ovvero delle macchine virtuali leggere che ci permettono di eseguire il nostro codice in modo isolato.
            \item Discorso di Logging e Monitoring, ovvero come tenere traccia di quello che succede nel nostro codice e come monitorare le performance del nostro modello.
        \end{itemize}
        \item Day 4
        \begin{itemize}
            \item Vedremo come si fa un packaging python, ovvero come creare un pacchetto python che possiamo distribuire e installare su altre macchine.
            \item Un accenno al discorso di AutoMl, ovvero come automatizzare il processo di addestramento del modello.
        \end{itemize}
\end{itemize}
\section{Valutazione del corso}
\begin{itemize}
    \item Bisognerà svolgere un progetto che affronti tutte le problematiche viste a lezione.
    \begin{itemize}
        \item Come si fa DEVOps, quindi utilizzo di Git, lint, unit test, etc.
        \item Training e test di un piccolo modello di Machine Learning.
        \item Si può implementare una piccola interfaccia di testing.
        \item Dockerizzazione del progetto (importante).
        \item GitHub Actions per il deploy del progetto.
        \item Monitoring e Logging del progetto.
    \end{itemize}
\end{itemize}

\section{Version Control Systems (VCS)}
Se dobbiamo sviluppare un software, dobbiamo tenere traccia delle modifiche che facciamo al codice. Per farlo utilizziamo i Version Control Systems (VCS), che ci permettono di tenere traccia delle modifiche, di collaborare con altri sviluppatori e di gestire le versioni del nostro codice.

\subsection{Source Control}
Cosa vuol dire Source Control? Vuol dire andare a sviluppare un progetto in modo che tutta questa struttura (progetti e file) sia organizzata. Se faccio una modifica e voglio tenerne traccia potrei essere tentato di fare copia e incolla, portandoci
ad un aumento esponenziale del numero di file e della loro gestione. I VCS
sono in grado di tenere traccia delle modifiche che facciamo il codice andando a fare:
\begin{itemize}
    \item \textbf{Restore} $\rightarrow$ ripristinare una versione precedente delle modifiche che stiamo facendo.
    \item \textbf{Check Out} $\rightarrow$ andare a vedere le modifiche che sono state fatte in un determinato file.
    \item \textbf{Recover} $\rightarrow$ recuperare un file che è stato cancellato.
    \item \textbf{Collaborate} $\rightarrow$ collaborare con altri sviluppatori, andando a gestire le modifiche che fanno loro e le modifiche che facciamo noi.
\end{itemize}

\subsection{Git}
Git è uno solo di questi VSC, ce ne sono molti altri ma utilizzeremo questo per il nostro progetto perchè è lo standard aziendale.
Per parlare di Git dobbiamo specificare e chiarire bene il concetto di Repository.
Che cos'è una Repository? 
\begin{itemize}
    \item una repository è l'archivio con tutti i cambiamenti che facciamo al nostro progetto.
    \item quando partiamo da una cartella vuota dobbiamo fare \textbf{git init} che dice a Git che questa cartella è una repository. 
    \item fatto ciò verrà creata una cartella chiamata .git, in cui ci sono tutti i file che gestiscono il version del sistema, i branch e via dicendo.
\end{itemize}

\subsection{Commit}
Il commit è l'azione che ci permette di salvare le modifiche che abbiamo fatto al nostro progetto. Quando facciamo un commit, Git salva lo stato attuale del nostro progetto e lo aggiunge alla cronologia delle modifiche.
Idealmente dovrebbe essere un singolo cambiamento, nella pratica delle cose i cambiamenti simili vengono accorpati. Tendenzialmente i cambiamenti si fanno quando siamo ad un buono stato
del progetto. Il commit quindi lo possiamo vedere come un check point, non deve essere per forza funzionante ma è buona prassi che lo sia.
Per far si che il commit sia valido deve avere:
\begin{itemize}
    \item \textbf{Autore: } chi ha fatto il commit, il nome e l'email.
    \item \textbf{Data: } (lo fa automaticamente Git)
    \item \textbf{Messaggio/Comment: } una breve descrizione del cambiamento che abbiamo fatto, che deve essere chiara e concisa.
    \item \textbf{Hashes: } un identificativo unico del commit, che viene generato automaticamente da Git. Questo hash è un numero esadecimale che identifica in modo univoco il commit.
\end{itemize}
La cosa più difficile da capire è il discorso della gestione delle versioni, ad esempio: Abbiamo 3 versioni e ne modifico 2 (file A e file C) ma non il file B.
Quindi poi quando faccio la versione successiva avrò la versione modificata dei miei file ma anche la versione originale di B. 
Git gestisce questo discorso facendo la copia e non il delta.
Il commit crea una catena di commit, che sono collegati tra loro. Ogni commit ha un hash che lo identifica in modo univoco e che è collegato al commit precedente.
Come facciamo nell'effettivo questo commit? Lo si fa tramite una \textbf{staging area :}
\begin{itemize}
    \item Prima di fare il commit dobbiamo dire a Git quali file vogliamo includere nel commit.
    \item La staging area è un piccolo container virtuale in cui buttiamo dentro cose finchè non abbiamo finito di lavorare, facendo un nuovo commit.
    \item \textbf{Nota: }creare un \textcolor{red}{nuovo file} è un cambiamento al codice. Ogni volta che creaiamo un nuovo file dobbiamo aggiungerlo manualmente, altrimenti lui non sa che esiste.
\end{itemize}
Se non voglio aggiungere qualcosa ho varie opzioni:
\begin{itemize}
    \item Non lo metto nella staging area, ma è un metodo poco pratico, che rischia di portarci all'errore.
    \item \textbf{gitignore} $\rightarrow$ è un file (.gitignore) che contiene all'interno tutti i nomi dei file e le cartelle che vogliamo ignorare. Tendenzialmente si utilizza per:
    \begin{itemize}
        \item non avere all'interno file di compilazione (poco utili nel source control).
        \item file troppo grandi (5-10 MB), non vogliamo appesantire la Repository.
        \item checkpoint dei modelli, datasets, zips, etc.
    \end{itemize}
\end{itemize}
Esempio di file \textbf{.gitignore:}
\begin{lstlisting}
    #Ignore data files
    #questo ignora tutta la cartella,
    #non comparira' mai sul source control una cartella 
    #chiamata data.
    data/

    #!data/.keep    # <-----this won't work, 
    #because we told git to ignore THE data folder
    #questo serve perche' a volte noi non vogliamo caricare 
    #la cartella, 
    #ma abbiamo bisogno della cartella come placeholder.


    #Ignore all files INSIDE checkpoints
    checkpoints/**

    #the following line will work, because we are 
    #telling git to ignore 
    #the checkpoints folder, but NOT the .keep file inside it
    !checkpoints/.keep

    #Ignore all .pth files
    #pth e' uno dei framework di deep learning piu' usati, 
    #e' un formato di file che contiene i pesi del modello e
    #le informazioni necessarie per ricreare il modello.
    *.pth

    #Ignore all .pyc files
    #pyc sono file intermedi di compilazione di python, 
    #ovvero file che python crea quando eseguiamo uno script.
    *.pyc  

    #Ignore pycache files
    #pycache e' una cartella che contiene i file .pyc
    __pycache__/
\end{lstlisting}

\subsection{Git CLI vs IDE}
Git può essere utilizzato sia come command line ma si può utilizzare anche un IDE, noi utilizzeremo Visual Studio Code (non è propriamente un'IDE bensi un editor di testo).
L'interfaccia di Visual Stdio Code (come per qualsiasi IDE) può essere molto utile per l'utente.

\subsection{Install requirements}
\begin{itemize}
    \item \textbf{Python: } si può utilizzare la versione di default di python preinstallata nel sistema, lo stesso vale per Ubuntu
    \item \textbf{Miniconda: } è una versione di python che fa anche la gestione degli ambienti virtuali, che sono delle istanze di python che possiamo utilizzare per isolare i nostri progetti. Ci permette di avere differenti interpreti di diverse versioni di python.
    \item \textbf{Visual Studio Code: } come detto prima è un editor di testo che possiamo utilizzare per scrivere il nostro codice. Ha un sacco di estensioni che ci permettono di utilizzare Git, Python, etc.
    \item \textbf{Git: } è il software che ci permette di utilizzare Git, ovvero il VCS che abbiamo visto prima. Possiamo installarlo tramite il package manager del nostro sistema operativo (apt per Ubuntu, brew per MacOS, etc).
\end{itemize}

\subsection{Visual Studio Code + Git}
Visual Studio Code ha un'integrazione con Git che ci permette di utilizzare Git direttamente dall'editor. Possiamo fare i commit, le modifiche, le pull request, etc. direttamente dall'editor senza dover utilizzare la command line.
\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{Picture1.png}
  \label{etichetta1}
\end{figure}
\begin{center}
$\Uparrow$
\end{center}
Questa è la "Home Page di Visual Studio Code", dove possiamo vedere i file che abbiamo aperto, le modifiche che abbiamo fatto, i commit che abbiamo fatto, etc.
Da qui tramite i pulsanti cmd+j possiamo aprire il terminale interno di Visual Studio Code.
\begin{center}
$\Downarrow$
\end{center}
\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{Picture2.png}
  \label{etichetta2}
\end{figure}
Ora nel terminale digitiamo il comando \framebox{mkdir git\_tests} per creare una cartella di prova chiamata appunto \textbf{git\_tests}.
Per aprire la cartella in questione in alto a destra dovremo selezionare \textbf{Open Folder} e selezionare la cartella che abbiamo appena creato.
Ora inizializziamo la repository vuota con il comando sul terminare \framebox{git init}. Noi questa cartella non la vediamo da VSC, nell'explorer possiamo vederla e troveremo dentro dei log, dei riferimenti ai commit e molto altro. Per esempio c'è il file config dove troviamo le informazioni del nostro utente, il file HEAD che ci dice a quale branch siamo collegati, etc.
Iniziamo creando un primo file python::
\begin{center}
    $\Downarrow $
\end{center}
\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{Picture3.png}
  \label{etichetta3}
\end{figure}
Notiamo subito delle modifiche nella nostra schermata di VSC. Questi simboli hanno un significato diverso in base a come Git sta trattando il nostro file:
\begin{itemize}
    \item \textbf{A} $\rightarrow$ il file è stato aggiunto alla staging area, quindi è pronto per essere committato.
    \item \textbf{M} $\rightarrow$ il file è stato modificato, quindi è stato cambiato rispetto all'ultima versione.
    \item \textbf{D} $\rightarrow$ il file è stato cancellato, quindi non esiste più nella repository.
    \item \textbf{U} $\rightarrow$ il file è in uno stato di uncommitted changes, ovvero non è stato ancora aggiunto alla staging area.
    \item \textbf{C} $\rightarrow$ il file è in uno stato di conflicted changes, ovvero ci sono dei conflitti tra le modifiche che abbiamo fatto e quelle che sono state fatte da altri sviluppatori.
    \item \textbf{R} $\rightarrow$ il file è stato rinominato, quindi il nome del file è stato cambiato rispetto all'ultima versione.
    \item \textbf{S} $\rightarrow$ il file è stato spostato, quindi il file è stato spostato in un'altra cartella rispetto all'ultima versione.
    \item \textbf{T} $\rightarrow$ il file è cambiato da symlink a file regolare, o viceversa.
\end{itemize}
Aprendo sulla sinistra il source control possiamo vedere la parte di gestione di Git:
\begin{center}
    $\Downarrow$
\end{center}
\begin{figure}[H]
  \centering
  \includegraphics[width=0.4\textwidth]{Picture4.png}
  \label{etichetta4}
\end{figure}
\begin{center}
    $\Downarrow$
\end{center}
\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{Picture5.png}
  \label{etichetta5}
\end{figure}
Qui vediamo che c'è la sezione \textbf{Changes} che ci mostra i cambiamenti che stiamo facendo, ed il pulsante \textbf{Commit} che serve appunto per creare un commit.
Altrettanto importante è il pulsante \textbf{+} che ci permette di mettere in stage.
Altro comando importante è $\circlearrowleft $ che ci permette di cancellare e fare undo dei cambiamenti, in questo caso con il file non tracciato (U), il file viene cancellato.
Prima di fare qualisasi cosa dobbiamo configurare il nostro utente, per farlo dobbiamo aprire il terminale e digitare i seguenti comandi:
\begin{center}
    $\Downarrow $
\end{center}
\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{Picture6.png}
  \label{etichetta6}
\end{figure}
Queste credenziali non devono essere per forza vere, serve solo a git per la stringa di commit.
I cambiamenti possono essere fatti sul singolo file o su tutti:
\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\textwidth]{Picture7.png}
  \label{etichetta7}
\end{figure}
\noindent Così facendo tutti i cambiamenti vengono messi in stage area.
Vedremo quindi di fianco al file il simbolo \textbf{A} che indica che il file è stato aggiunto alla staging area.
A questo punto dovremo fare il commit, per farlo dobbiamo scrivere un messaggio che descriva il cambiamento.
\begin{center}
    $\Downarrow$
\end{center}
\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{Picture8.png}
  \label{etichetta8}
\end{figure}
Si creerà un graph che ci permette di vedere la lista concatenata dei commit che abbiamo fatto.
\begin{center}
    $\Downarrow$
\end{center}
\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{Picture9.png}
  \label{etichetta9}
\end{figure}
In ciascuno di questi commit c'è un identificativo unico che è l'hash, che ricordiamo è un valore esadecimale che rappresenta in modo univoco il commit.
Proviamo ad aggiungere delle modifiche al nostro file:
\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{Picture10.png}
  \label{etichetta10}
\end{figure}
\noindent Notiamo subito i cambiamenti nell'explorer di VSC, prima cosa vediamo una linea blu su VSC con le modifiche, facendoti vedere anche com'era prima, è passato poi da U ad M essendo stato modificato.
Passo successivo lo mettiamo in stage area, cliccando sul simbolo \textbf{+} e poi facciamo il commit.
\begin{figure}[H]
  \centering
  \includegraphics[width=0.3\textwidth]{Picture11.png}
  \label{etichetta11}
\end{figure}
\noindent Notiamo subito che si è spostato il master, vuol dire che adesso la cosiddetta \textbf{HEAD} del nostro progetto è il master.

\subsection{VSC + Git - Collaboration}
Collaborare con altri sviluppatori è una delle cose più importanti quando si sviluppa un progetto. Git ci permette di farlo in modo semplice e veloce.
Per pushare la repository dobbiamo cliccare sul pulsante \textbf{Publish Branch}, decidendo se renderla pubblica o privata.
Una volta pubblicata su GitHub, nella sezione \textbf{Settings} possiamo aggiungere altri collaboratori, che potranno accedere alla nostra repository e fare modifiche.
\begin{figure}[H]
  \centering
  \includegraphics[width=1\textwidth]{Picture12.png}
  \label{etichetta12}
\end{figure}
\noindent Sempre su GitHub una volta fatto il commit vedremo i vari hash, chi l'ha generato ed altre informazioni che vedremo in seguito.

\subsection{Branches}
Sono dei set isolati di commit, che ci permettono di lavorare in maniera separata con il codice. Idealmente il branch serve da feature, ovvero per sviluppare una nuova funzionalità senza intaccare il codice principale.
Questo è molto importante per fare bug fixing, code refactoring, etc.

\subsection{Merging}
Fatto il mio branch, dobbiamo rimettere "in ordine" il codice nel nostro tree, questo può essere fatto in automatico se non abbiamo fatto modifiche contemporanee con altre persone mandandole in conflitto, oppure sono state fatte sullo stesso file ma in punti diversi.
\newline Oppure si fa manualmente:
\begin{itemize}
    \item Modifico i file che hanno conflitto.
    \item Si rimette in stage area.
    \item Si fa un nuovo commit.
    \item Si riprova il merge.
\end{itemize}
Tutto questo va fatto ripetutamente finchè non si risolvono tutti i conflitti.

\section{DevOps vs MLOps}
\textbf{DevOps} è un paradigma per cui si fa molto rapidamente lo sviluppo, il rilascio del codice ed il controllo di produzione.
Generalmente si divide in 2 parti:
\begin{enumerate}
    \item \textbf{Development (sviluppo): }dove si fa la pianificazione del codice, si affronta il problema e capisco come risolverlo. Scrivo il codice, faccio la build ed infine faccio i test.
    \item \textbf{Ops: }in questa fase si fa il rilascio, decidendo come distribuirlo e su quale piattaforma. Facendo monitoring e dicendo come farlo funzionare.
\end{enumerate}
\textbf{MLOps} è un po' diverso, abbiamo sempre il DevOps come parte integrante dello sviluppo di un'applicazione. Differisce però perchè che c'è una parte in più che è il Machine Learning.
Nel \textbf{Machine Learning} la parte importante sono i \textbf{dati} ed il \textbf{modello}. 
Concettualmente sono molto simili ma ci sono delle differenze nei punti chiave, per esempio nel deployment abbiamo, nel DevOps abbiamo del codice che una volta che viene eseguito e gestito nell'ambito di sviluppo, verrà wrappato 
e in un eseguibile e deve essere validato su dei test (unit/integration). Nell'MLOps abbiamo un elemento in più nel Development che è la gestione e la creazione del modello di Machine Learning per l'artefatto\footnote{Un artefatto, in questo contesto, è il risultato finale del processo di sviluppo: può essere un modello addestrato, un pacchetto software, uno script o qualsiasi altro prodotto generato dal workflow.} finale.
Qui la validazione viene fatto sui dati, per vedere come performa il modello, questo generalmente va fatto mentre si sviluppa il software e non durante la consegna al cliente.
Il \textbf{Version Control} tra DevOps ed MLOps differisce perchè:
\begin{itemize}
    \item Nel \textbf{DevOps} abbiamo $\rightarrow$ codice e l'artefatto/eseguibile generato.
    \item Nell' \textbf{MLOps} abbiamo $\rightarrow$ abbiamo comunque del codice, abbiamo però in più la gestione degli iperparametri, la gestione delle metriche, i dati train/test ecc.
\end{itemize}
I concetti chiave del \textbf{DevOps} sono:
\begin{itemize}
    \item \textbf{Continuous integration (CI): } Quando il codice è stato scritto ed è in una versione funzionante, automaticamente vengono eseguiti tutti quanti gli unit test del caso, per verificare che il mio codice sia consistente e che non abbia rotto niente di quello che c'era prima. $\leftarrow$ \textcolor{red}{\textbf{Importante!}}
    \item \textbf{Continuous delivery (CD): } Fatto e testato il codice, una volta che tutto funziona lo mando in deploy, come faccio a farlo? C'è una parte del codice che dice che, ogni volta che ho controllato che tutto funzioni, manda tutto ad un server remoto mettendosi da solo in produzione.
    \item \textbf{Microservices: } Questo è un concetto, tendenzialmente vorrei lavorare con dei software che sono dei microsevizi, che non hanno nessuna dipendenza. (Magari vogliamo che ogni servizio del backend sia un microservizio separato quindi, una parte con autenticazione, una parte che gestisca una cosa e via dicendo).
    \item \textbf{Infrastructure as Code (IaC): } Sono tutti i sistemi definiti lato codice con degli script che costruiscono l'intero stack. (esistono software tipo Terraform che ci aiutano in questo)
    \item \textbf{Monitoring } and Instrumentation: Serve per prendere decisioni per quanto riguarda l'affidabilità e la sicurezza del codice.
\end{itemize}

\subsection{Passaggio dal DevOps al MLOps}
Prima di partire con il Machine Learning dobbiamo capire bene il DevOps (base di partenza da cui costruiamo tutto il resto). 
Nella pratica questo si traduce in implementazione di \textbf{Continuous integration e Continuous delivery}, tutte le altre parti sono molto meno critici, però esistono ed è bene sapere come vengono affrontati.

\subsubsection{Continuous Integration}
Voglio fare automaticamente test di integrazione o test di unità. Per esempio voglio vedere il lint\footnote{Python utilizza il duck-typing, cioè il tipo di una variabile viene determinato dal suo comportamento e non dalla sua dichiarazione esplicita. Questo può portare a errori difficili da individuare. Il \textbf{lint} è uno strumento che analizza il codice sorgente per individuare errori di sintassi, stile o potenziali bug, aiutando a mantenere il codice più pulito e conforme alle best practice.}
o il test, ecc.
Ecco un esempio:
\begin{lstlisting}
    name: Deploy Python 3.5
    on: [Push]
    jobs:
        build:
            runs-on: ubuntu-latest
            steps:
            - uses: actions/checkout@v4
            - name: Set up Python 3.12
                uses: actions/setup-python@v5
                with:
                    python-version: '3.12'
            - name: Install dependecies
                run: |
                    make install
            - name: Lint
                run: |
                    make lint
            -name: Test
                run: |
                    make test
\end{lstlisting}
\newpage
\subsubsection{Continuous Delivery}
Una persona scrive un pezzo di codice, lo manda su GitHub. Nel momento in cui faccio subito un commit GitHub farà partire da solo tutto ciò che serve per fare \textbf{delivery} $\downarrow $
\begin{figure}[H]
  \centering
  \includegraphics[width=1\textwidth]{Picture13.png}
  \label{etichetta13}
\end{figure}
\noindent Quindi manda in poduzione, carica un po' di volte il test per fare delle prove gestisce la parte cloud e via dicendo. Gestisce lui le cose automaticamente le cose (in base a quanto si paga).
\subsection{MLOps}
MLOps è un estensione del DevOps, l'unica cosa che dovremo fare in più è gestire la parte di ML. 
In un modo molto pratico MLOps funziona in questo modo:
\newline
Prendo il codice, faccio il deploy e lo addestro, vedo come va il modello e se è funzionante, se non funziona riaddestro il modello e ricomincio. Tutto questo viene fatto in loop finchè non è funzionante.
\newline
In MLOps c'è un 25\% importante che riguarda tutta la struttura della repository, c'è poi un'altra parte importante che è il buiseness (nel nostro progetto naturalmente non ci sarà), abbiamo poi un 25\% di dati, che sono fondamentali per il nostro lavoro, tante volte questi dati dobbiamo saperli ricavare da soli. Abbiamo in fine un 25\% di modello, la scelta ed il design del modello sono molto importanti. 
\vspace{1em}
La prima cosa da fare è installare un virtual environment (qui verrà spiegato con venv, metodo più didattico, nel progetto useremo miniconda).
Iniziamo digitando sul terminale (che ricordiamo si apre con ctrl+j su VSC), e digitiamo \framebox{python -m venv venv}. 
Una volta fatto ciò (dopo aver installato l'estensione per python su VSC), schiacciano ctrl+p si aprirà una schermata in cui digitare 
\textbf{>interpreter}.
\begin{center}
    $\Downarrow$
\end{center}
\begin{figure}[H]
  \centering
  \includegraphics[width=1\textwidth]{Picture14.png}
  \label{etichetta14}
\end{figure}
Selezionando \textbf{Python: Select Interpreter} si aprirà questa tendina dove selezioniamo \textbf{Workspace}.
\begin{center}
    $\Downarrow$
\end{center}
\begin{figure}[H]
  \centering
  \includegraphics[width=1\textwidth]{Picture15.png}
  \label{etichetta15}
\end{figure}
Questa cartella ovviamente non la vorremo sul Source Control.
Dobbiamo poi controllare se l'interprete che abbiamo caricato sia quello corretto, si può fare in 2 modi:
\begin{itemize}
    \item \textbf{Linux: } (Sul terminale) \framebox{which "nome\_eseguibile"} $\rightarrow$ ci dirà dove si trova quell'eseguibile nel sistema operativo.
    \item \textbf{Windows: } (Sul terminale) \framebox{where.exe "nome\_eseguibile"} $\rightarrow$ dovessero esserci più risultati sappiamo che il primo è quello usato principalmente.
\end{itemize}
Per attivare manualmente l'interprete manualmente i comandi da terminale sono:
\begin{itemize}
    \item \textbf{Linux/macOS: } \framebox{source venv/bin/activate}
    \item \textbf{Windows (PowerShell): } \framebox{\texttt{.\textbackslash venv\textbackslash Scripts\textbackslash Activate.ps1}}
    \item \textbf{Windows (CMD): } \framebox{\texttt{venv\textbackslash Scripts\textbackslash activate.bat}}
\end{itemize}

\subsection{Esempio (e corso accelerato di Python)}
Definiamo una repo GitHub in questo modo:
\begin{itemize}
    \item MakeFile
    \item requirements.txt
    \item hello.py
    \item test\_hello.py
    \item venv
\end{itemize}

\subsubsection{MakeFile}
Utilissimo per velocizzare i processi di produzione. Ecco un esempio di MakeFile 
\begin{center}
    $\Downarrow$
\end{center}
\begin{figure}[H]
  \centering
  \includegraphics[width=1\textwidth]{Picture16.png}
  \label{etichetta16}
\end{figure}

Il Makefile contiene tre target che automatizzano operazioni di sviluppo Python:

\begin{enumerate}[leftmargin=*,label=\textbf{\arabic*.}]
    \item \texttt{install}: Aggiorna pip e installa le dipendenze
    \begin{lstlisting}
    python -m pip install --upgrade pip && pip install -r 
    requirements.txt
    @echo "Installation complete. 
    You can now run the project."
    \end{lstlisting}
    \begin{itemize}
        \item \texttt{python -m pip install --upgrade pip}: Aggiorna pip all'ultima versione
        \item \texttt{pip install -r requirements.txt}: Installa tutti i pacchetti specificati nel file \texttt{requirements.txt}
        \item \texttt{@echo}: Mostra un messaggio di conferma (il simbolo \texttt{@} evita di stampare il comando stesso)
    \end{itemize}
    
    \item \texttt{list}: Esegue il linting del codice
    \begin{lstlisting}
    pylint --disable=R,C hello.py
    @echo "Linting complete. No issues found."
    \end{lstlisting}
    \begin{itemize}
        \item \texttt{pylint}: Strumento di analisi statica per Python
        \item \texttt{--disable=R,C}: Disabilita i controlli per:
        \begin{itemize}
            \item \texttt{R} (Refactor suggestions)
            \item \texttt{C} (Convention violations)
        \end{itemize}
        \item Analizza specificamente il file \texttt{hello.py}
    \end{itemize}

    \item \texttt{test}: Esegue i test unitari con coverage
    \begin{lstlisting}
    python -m pytest -vv --cov=hello test_hello.py
    @echo "Testing complete. All tests passed."
    \end{lstlisting}
    \begin{itemize}
        \item \texttt{python -m pytest}: Esegue pytest come modulo
        \item \texttt{-vv}: Modalità verbose (doppia verbosità)
        \item \texttt{--cov=hello}: Misura la coverage del modulo \texttt{hello}
        \item \texttt{test\_hello.py}: Esegue i test contenuti in questo file
    \end{itemize}
\end{enumerate}

\subsection*{Note importanti}
\begin{itemize}
    \item Nei Makefile, i comandi devono usare tabulazioni (non spazi) per l'indentazione
    \item Il target \texttt{1 install} è sintatticamente errato (i nomi non possono contenere spazi)
    \item I separatori \texttt{---} non sono standard nei Makefile
\end{itemize}

Per eseguire un MakeFile dobbiamo digitare questi comandi sul terminale:
\begin{itemize}
    \item \textbf{Linux: } $\rightarrow $ \framebox{\texttt{sudo apt update \&\& sudo apt install make}}
    \item \textbf{Windows: } $\rightarrow $ \framebox{\texttt{winget install -e --id GnuWin32.Make}}
    \item \textbf{Windows: } $\rightarrow $ \framebox{\texttt{choco install make}}
\end{itemize}
Facciamo poi \textbf{make install}, il quale aggiorna pip all'ultima versione ed automaticamente fa tutto ciò che c'è scritto.
\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{Picture21.png}
  \label{etichetta21}
\end{figure}
\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{Picture22.png}
  \label{etichetta22}
\end{figure}
A questo punto facciamo il test \textbf{make lint}: 
\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{Picture23.png}
  \label{etichetta23}
\end{figure}
Facciamo poi il \textbf{make test}: 
\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{Picture24.png}
  \label{etichetta24}
\end{figure}

\subsubsection{Requirements.txt}
\'{E} una lista di nomi di package.
\begin{lstlisting}
    pylint
    pytest
    pytest-cov
\end{lstlisting}
Questi sono quelli fondamentali per questo test.

\subsubsection{Test script} 
Semplice script:
\begin{lstlisting}
    def add(x: int, y: int) -> int:
        """Add two numbers."""
        return x9y
    print(add(1,2)) #Output:3
\end{lstlisting}

\begin{lstlisting}
    from hello import add

    def test_add():
        assert add(1, 2) == 3
        assert add(-1, 1) == 0
        assert add(0, 0) == 0
        assert add(-1, -1) == -2
        assert add(1000000, 2000000) == 30000000
\end{lstlisting}
Tutti le funzioni che testano un metodo devono chiamarsi \textbf{test\_} e poi il nome, altrimenti non vengono riconosciuti.

Il gitignore di questo esempio sarà: 
\begin{lstlisting}
    .pytest_cache/
    venv/
    .coverage

    *.pyc 
    __pycache__/
    *.pyo
    *.pyd
\end{lstlisting}

\newpage
\subsubsection{Continous Integration}
Ora dobbiamo creare le seguenti cartelle: .github $\rightarrow$ workflow $\rightarrow$ file chiamato \textbf{deploy.yml}
\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{Picture25.png}
  \label{etichetta25}
\end{figure}
Spieghiamo brevemente: 
\begin{itemize}
    \item name: $\rightarrow$ semplicemente il nome che gli diamo noi 
    \item on: [push] $\rightarrow$ faccio un commit e poi fa il push su github e lo esegue 
    \item runs-on: ubuntu-latest $\rightarrow$ esegue su sistema operativo ubuntu
    \item uses: action/checkout@v4 $\rightarrow$ scarica la repositori 
    \item name: Set up Python 3.12 $\rightarrow$ scarica python in quella versione e si crea l'ambiente
    \item quelle sotto sono le dipendenze fatte invece automatico dal MakeFile
\end{itemize}
Una volta creato questo file faccio Publish Branch, il commit della action lo vediamo sotto actions nella nostra pagina GitHub (chiamata build).

\section{Data Science and EDA}
Il Machine Learning parte dai dati, dobbiamo quindi essere sicuro di cosa parlano i dati che stiamo trattando, utilizzando male i dati
potrebbero portare ad un modello funzionante ma con una comprensione del mondo esterna sbagliata.
\textbf{Data Science}
\begin{itemize}
    \item Tutto in AI parte dai dati, senza di questi non possiamo addestrare niente. I modelli di AI prendono un esempio, facendo un'ipotesi e andando a tentativi finchè non trovano quella corretta. Contninuano poi a lavorare su quella corretta fino ad arrivare a convergenza. 
    \item Questi modelli sono \textbf{Statistici}, per lavorare bene nell'ambito del machine learning la comprensione statistica è fondamentale.
    \item I dati hanno una forma (rappresentazione), una buona rappresentazione per i dati naturali è la Gaussiana. I modelli imparano dai dati (imparano una sorta di rappresentazione statistica).
    \item Se ho dei dati di scarsa qualità (es: introduzione di un bias senza saperlo), avremo, come detto prima, un modello pessimo.
\end{itemize}
Quando non abbiamo i dati possiamo quindi ricadere in alcuni problemi:
\begin{itemize}
    \item Rumore quindi notazioni poco corrette.
    \item Pochi dati perchè magari alcune classi sono poco rappresentate (a volte è un limite economico). $\rightarrow$ Sottorappresentazione
    \item I Biases possono crearci diverse tipologie di problemi, che a volte, senza l'acquisizione di altri dati, sono irrisolvibili.
    \item Possiamo avere un dataset sbilanciato, il che porta ad avere dei dati poco o troppo rappresentati.
    \item Ce ne sono anche altri di cui non parleremo.
\end{itemize}

Data Science Workflow:
\begin{itemize}
    \item \textbf{Input}: si parte naturalmente dai dati.
    \item \textbf{Data Cleaning}: prendiamo i dati e li controlliamo TUTTI, verificando che non ci siano errori (annotazioni sbagliate, immagini rotte, ecc). Lo si fa per tutti ma non uno per uno, lo faremo con metodi statistici.
    \item \textbf{Data Representation}: dobbiamo capire come rappresentare i dati, ad esempio tante immagini o tabelle con migliaia di colonne\dots Questa rappresentazione va bene per il Machine Learning (tirando fuori le informazioni che sappiamo essere importanti per la risoluzoine del nostro problema), di meno per il Deep Learning.
    \item \textbf{Preprocessing}: si occupa di scalare i nostri dati (che si trovano in un dominio numerico di qualunque tipo) e li dobbiamo portare in qualcosa di interpretabile facilmente dal mio sistema (in un sistema di Machine Learning si dice che normalizzo i dati rispetto ad un intervallo).
    \item \textbf{Model Training}: divido il mio modello in training e testing:
    \begin{itemize}
        \item \textbf{training}: il modello prende i dati, ci lavora sopra dandoci in output un risultato.
        \item \textbf{testing}: questa parte non dovrà mai essere vista dal modello in training, vengono usati solo per capire come potrebbe performare nella realtà.
    \end{itemize}
    \item \textbf{Performance Measurements}: alla fine dovremo misurare statisticamente le performance, in base ai risultati capiamo se il modello ha imparato bene o meno le informazioni date tramite i dati.
\end{itemize}

\subsection{Data types: quantitative}
\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{Picture1.jpg}
  \label{etichetta26}
\end{figure}
Ci sono diverse tipologie di dati e vengono trattati in modo diversi:
\begin{itemize}
    \item \textbf{Quantitativi: } Sono i dati che posso misurare ed alla quale posso attribuire un valore.
    \begin{itemize}
        \item Continui: Sono dati "infiniti", altezza, orario \dots
        \item Discreti: Numeri interi o comunque finiti, con una certa separazione nello spaio in cui vivono.
    \end{itemize}
    \item \textbf{Qualitativi: } Sono dati che non sono direttamente rappresentabili tramite misurazioni.
    \begin{itemize}
        \item Strutturati: Ad esempio tabelle, stringhe \dots
        \item Non Strutturati: Dati di partenza tipo sorgenti testuali, codice delle applicazione \dots
    \end{itemize}
    \item \textbf{Misure: } 
    \begin{itemize}
        \item Nominali: Ad esempio maschio/femmina, tutte cose categorizzabili.
        \item Ordinali: Ad esempio la taglia di una maglia \dots
        \item Intervalli: Ad esempio temperatura misurata in gradi celsius \dots
        \item Di rapporto: Uguali agli intervalli ma hanno lo zero assoluto.
    \end{itemize}
\end{itemize}

\subsection{Exploratory Data Analysis (EDA)}
Questa parte si occupa di capire i dati che stiamo utilizzando, tramite strumenti di visualizzazione principalmente qualitativi (visualizzazione quindi grafici ecc.) o quantitativi (enumarazione).
I dati generalmente sono sporchi (perchè costa molto sistemare i dati), per questo è molto importante un'analisi precisa dei nostri dati e di come utilizzarli.
Gli strumenti principali di visualizzazione sono:
\begin{itemize}
    \item \textbf{Graphical Tools:}
    \begin{itemize}
        \item \textbf{Box Plot: }Strumento grafico per mostrare una di certa variabile la sua località, la sua distribuzione, quanto può essere sbilanciata rispetto ad un certo valore e quindi avere tramite quartili una rappresentazione del dato più uniforme possibile.
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.5\textwidth]{Picture2.jpg}
            \label{etichetta27}
        \end{figure}
        \item \textbf{Histogram: }Strumento di visualizzazione della distribuzione di dati quantitativi, nell'immagine sottostante vediamo la distribuzione della codifica del colore del pixel. Si può usare per altre cose ad esempio le variabili categoriche o altri tipi di variabili in cui è utile questo tipo di visualizzazione grafica.
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.5\textwidth]{Picture3.jpg}
            \label{etichetta28}
        \end{figure}
        \item \textbf{Run Chart: }\'{E} utile se abbiamo delle serie temporali, si riesce a fare un'analisi di come potrebbe essere il nostro dato lungo una variabile indipendente.
         \begin{figure}[H]
            \centering
            \includegraphics[width=0.5\textwidth]{Picture4.jpg}
            \label{etichetta29}
        \end{figure}   
        \item \textbf{Scatter Plot: }Nella quale decidicamo 2 variabili e si fa un plot di quest'ultime lungo un piano cartesiano. Non ci fa vedere solo la quantità delle 2, ci fa vedere il loro rapporto, come evolvono, in pratica mette in relazione tra di loro i dati correlati.
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.4\textwidth]{Picture5.jpg}
            \label{etichetta30}
        \end{figure}
    \end{itemize}
\end{itemize}
Una volta che ho rappresentato i miei dati, una cosa che può risultare utile è la riduzione della dimensionalità. Alcuni dati
potrebbero essere difficili da rappresentare (ad esempio ha più di 3 dimensioni), quello che possiamo, utilizzando gli stessi strumenti di prima, ridurre la dimensionalità e passare
magari da 4 dimensioni ad 1. Lo faccio tramite una tecnica chiamata \textbf{PCA}. Lo scopo di questa tecnica è: prendo la dimensione massima di massima diffusione (spread) del mio dato,
e da questo vado ad estrarre i più alti autovalori della matrice di covarianza e ottengo PC1, PC2. Una volta che le ho posso tagliare le misure che non sono 
di principale rilevanza, tenendo quindi solo quelle di principale estensione.
\begin{center}
    $\Downarrow$
\end{center}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{Picture6.jpg}
    \label{etichetta31}
\end{figure}
Ricordiamoci che ogni ad ogni informazioni che tagliamo perdiamo precisione quindi questo processo va fatto con cautela e solo quando serve. Ad esempio quando prendiamo dei dati che possono essere intuitivi per noi ad alte dimensioni,
ad esempio una scatola (altezza - larghezza - volume - profondità) e la trasformo soltanto in 2 dimensioni che in quel momento possono esserci utili, ad esempio il peso e magari la profondità.

\newpage
\subsection{Preprocessing in DL}
Il preprocessing nelle reti neurali è abbastanza limitato, questo perchè l'estrazione delle feature e le informazioni principali e più importanti del mio dato lo fa direttamente il modello quando viene addestrato.
Un sistema di Deep Learning funziona così:
\begin{enumerate}
    \item Prendo un dato.
    \item Da questo dato estraggo le informazioni più importanti di questo dato.
    \item Dalle informazioni faccio la classificazione o il mio task finale.
\end{enumerate}
Che informazioni estraggo? Ad esempio da un immagine come faccio a sapere cosa devo andare a guardare ? Lo sa già il modello, non glielo dobbiamo dire noi.
Con il preprocessing noi non facciamo molto, solitamente è solo un problema strettamente numerico, tutto il resto è automatico.

\subsection{Model Training \& Selection}
I sistemi di deep learning si addestrano con un framework, noi useremo Pytorch che è quello che da più controllo in assoluto tra quelli disponibili. 
Partiamo con un semplice script di python $\Downarrow$
\begin{lstlisting}[language=Python, basicstyle=\ttfamily\footnotesize, breaklines=true, frame=single]
"""
Install requirements with:
pip install torch torchvision scikit-learn matplotlib seaborn tqdm
"""

import time 
import torch 
import torch.nn as nn 
import torchvision 
import torchvision.transforms as transforms
from tqdm import tqdm
import numpy as np 
import matplotlib.pyplot as plt 
import seaborn as sns 
from sklearn.metrics import classification_report, confusion_matrix 
# Automatically download and load MNIST data 
transform = transforms.Compose([transforms.ToTensor()])
train_set = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)
test_set = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)

train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_set, batch_size=1000, shuffle=False)
\end{lstlisting} 
$\Uparrow$ Questo codice configura l'ambiente per addestrare e valutare un modello di machine learning sul dataset MNIST (numeri scritti a mano) utilizzando PyTorch. Ecco una spiegazione dettagliata:
\begin{itemize}
    \item Installazione delle dipendenze:
    \begin{lstlisting}[language=Python, basicstyle=\ttfamily\footnotesize, breaklines=true, frame=single]
        pip install torch torchvision scikit-learn matplotlib seaborn tqdm
    \end{lstlisting}
    Installa i pacchetti necessari:
    \begin{itemize}
        \item torch: Libreria principale per il deep learning.
        \item torchvision: Dataset e strumenti per computer vision.
        \item scikit-learn: Metriche di valutazione.
        \item matplotlib e seaborn: Visualizzazione dati.
        \item tqdm: Barre di avanzamento.
    \end{itemize}
    \item Importazione delle librerie:
    \begin{lstlisting}[language=Python, basicstyle=\ttfamily\footnotesize, breaklines=true, frame=single]
        import time 
        import torch 
        import torch.nn as nn 
        import torchvision 
        import torchvision.transforms as transforms
        from tqdm import tqdm
        import numpy as np 
        import matplotlib.pyplot as plt 
        import seaborn as sns 
        from sklearn.metrics import classification_report, confusion_matrix 
    \end{lstlisting}
    Importa tutti i moduli necessari per:
    \begin{itemize}
        \item Gestione tensori e reti neurali (torch, nn)
        \item Caricamento dati (torchvision)
        \item Preprocessing immagini (transforms)
        \item Visualizzazione (plt, sns)
        \item Valutazione del modello (classification\_report, confusion\_matrix)
    \end{itemize}
    \item Processing dei dati:
    \begin{lstlisting}[language=Python, basicstyle=\ttfamily\footnotesize, breaklines=true, frame=single]
        transform = transforms.Compose([transforms.ToTensor()])
    \end{lstlisting}
    Crea una trasformazione che:
    \begin{itemize}
        \item Converte le immagini in tensori PyTorch
        \item Normalizza automaticamente i pixel in intervallo [0, 1]
    \end{itemize}
    \item Caricamento del dataset MNIST:
    \begin{lstlisting}[language=Python, basicstyle=\ttfamily\footnotesize, breaklines=true, frame=single]
        train_set = torchvision.datasets.MNIST(
            root='./data', 
            train=True, 
            download=True,  # Scarica se non presente
            transform=transform
        )

        test_set = torchvision.datasets.MNIST(
            root='./data', 
            train=False, 
            download=True,
            transform=transform
        )
    \end{lstlisting}
    \begin{itemize}
        \item Training set: 60,000 immagini
        \item I dati vengono salvati in ./data
        \item Le immagini sono convertite in tensori automaticamente
    \end{itemize}
    \item Creazione dei DataLoader:
    \begin{lstlisting}[language=Python, basicstyle=\ttfamily\footnotesize, breaklines=true, frame=single]
        train_loader = torch.utils.data.DataLoader(
            train_set, 
            batch_size=64,  # 64 campioni per batch
            shuffle=True    # Mescola i dati ad ogni epoca
        )

        test_loader = torch.utils.data.DataLoader(
            test_set, 
            batch_size=1000,  # Batch piu grandi per test
            shuffle=False     # Non mescolare per valutazione
        )   
    \end{lstlisting}
    Funzionamento del DataLoader:
    \begin{table}[ht]
    \centering
    \caption{Confronto DataLoader Training vs Test}
    \label{tab:dataloader}
    \begin{tabular}{l >{\raggedright\arraybackslash}p{5cm} >{\raggedright\arraybackslash}p{5cm}}
    \toprule
    \textbf{Parametro} & \textbf{Training} & \textbf{Test} \\
    \midrule
    Batch size & 
    64 (dimensione ottimale per l'aggiornamento dei pesi) & 
    1000 (dimensione maggiore per valutazione più veloce) \\
    \addlinespace[0.3cm]
    Shuffle & 
    Attivato (mescola i dati ad ogni epoca per prevenire overfitting) & 
    Disattivato (mantiene l'ordine originale per valutazione consistente) \\
    \addlinespace[0.3cm]
    Funzione & 
    \multicolumn{2}{c}{Fornisce batch di dati al modello durante l'addestramento} \\
    \bottomrule
    \end{tabular}
    \end{table}
\end{itemize}
\newpage
Struttura complessiva del flusso dati $\Downarrow$
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{AA.png}
    \label{etichetta32}
\end{figure}
\vspace{3em}
\noindent Facciamo ora un po' di EDA, il codice riportato sotto è un'analisi molto semplice per capire cosa vuol dire lavorare con un dataset nuovo e capire cosa fa:
\begin{lstlisting}[language=Python, basicstyle=\ttfamily\footnotesize, breaklines=true, frame=single]
# Exploratory Data Analysis (EDA)
print("EDA Reports:")
print("- Train data size:", len(train_set))
print("- Test data size:", len(test_set))

# Check distribution of classes
labels = train_set.targets.numpy()
unique, counts = np.unique(labels, return_counts=True)
class_dist = dict(zip(unique, counts))
print("- Class distribution:", class_dist)

# Plot class distribution
plt.figure(figsize=(10,4))
sns.barplot(x=list(class_dist.keys()), y=list(class_dist.values()))
plt.title("Class distribution in training set")
plt.xlabel("Digit")
plt.ylabel("Count")
plt. show()

# Show sample images
examples = enumerate(train_loader)
batch_idx, (example_data, example_targets) = next(examples)
plt.figure(figsize=(10,4))
for i in range(10):
plt.subplot(2,5,i+1)
plt. imshow(example_data[i][0], cmap='gray', interpolation='none')
plt.title(f"Label: {example_targets[i].item()}")
plt.axis('off')
plt.tight_layout()

plt. show()
\end{lstlisting}
$\Uparrow$ Come detto prima questo codice esegue un'Analisi Esplorativa dei Dati (EDA) su un dataset di immagini (probabilmente MNIST o simile), utilizzando PyTorch e librerie di visualizzazione. Ecco una spiegazione dettagliata:
\begin{enumerate}
    \item Stampa delle dimensioni dei dataset:
    \begin{lstlisting}[language=Python]
        print("- Train data size:", len(train_set))
        print("- Test data size:", len(test_set))
    \end{lstlisting}
    Mostra il numero di campioni nel training set e nel test set. 
    
    Output esempio:
    \begin{itemize}
        \item Train data size: 60000
        \item Test data size: 10000
    \end{itemize}
    
    \item Analisi della distribuzione delle classi:
    \begin{lstlisting}[language=Python, basicstyle=\ttfamily\footnotesize, breaklines=true, frame=single]
        import numpy as np
        
        labels = train_set.targets.numpy()
        unique, counts = np.unique(labels, return_counts=True)
        class_dist = dict(zip(unique, counts))
        print("- Class distribution:", class_dist)
    \end{lstlisting}
    Questa parte di codice estrae le etichette (labels) del training set, conta le occorrenze di ogni classe (cifre da 0 a 9) e stampa la distribuzione in formato dizionario.
    
    Output esempio:
    \begin{itemize}
        \item Class distribution: \{0: 5923, 1: 6742, 2: 5958, 3: 6131, 4: 5842, 5: 5421, 6: 5918, 7: 6265, 8: 5851, 9: 5949\}
    \end{itemize}
    
    \item Grafico a barre della distribuzione:
    \begin{lstlisting}[language=Python, basicstyle=\ttfamily\footnotesize, breaklines=true, frame=single]
        import matplotlib.pyplot as plt
        import seaborn as sns
        
        plt.figure(figsize=(10, 6))
        sns.barplot(x=list(class_dist.keys()), y=list(class_dist.values()))
        plt.xlabel('Digit')
        plt.ylabel('Count')
        plt.title('Distribuzione delle classi nel dataset MNIST')
        plt.show()
    \end{lstlisting}
    Questa parte genera un grafico a barre che mostra il conteggio dei campioni per ogni classe, usa seaborn per una visualizzazione chiara e dà le etichette ``Digit'' all'asse x e ``Count'' all'asse y. È utile perché identifica sbilanciamenti tra le classi.
    
    \item Visualizzazione di immagini campione:
    \begin{lstlisting}[language=Python, basicstyle=\ttfamily\footnotesize, breaklines=true, frame=single]
        import matplotlib.pyplot as plt
        
        examples = enumerate(train_loader)
        batch_idx, (example_data, example_targets) = next(examples)
    \end{lstlisting}
    Carica un batch di dati dal DataLoader del training set e prende il primo batch (di solito 64 o 128 immagini).
    
    \begin{lstlisting}[language=Python, basicstyle=\ttfamily\footnotesize, breaklines=true, frame=single]
        plt.figure(figsize=(12, 6))
        for i in range(10):
            plt.subplot(2, 5, i+1)
            plt.imshow(example_data[i][0], cmap='gray')
            plt.title(f"Label: {example_targets[i].item()}")
            plt.axis('off')
        
        plt.tight_layout()
        plt.show()
    \end{lstlisting}
    Questa parte infine crea una griglia 2x5 con 10 immagini, mostra ogni immagine in scala di grigi (\texttt{cmap='gray'}), aggiunge l'etichetta come titolo. 
    \newline
    \texttt{example\_data[i][0]} seleziona il primo canale (immagini in bianco e nero).
\end{enumerate}
Facciamo vedere delle rappresentazioni visive del comportamento, ad esempio un possibile output della distribuzione può essere:
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{Picture11.jpg}
    \label{etichetta33}
\end{figure}
\noindent Da qui vediamo che ad esempio abbiamo un piccolo sbilanciamento della classe 1, notiamo comunque che è ben bilanciato.
Andiamo a vedere quindi qualche esempio (10 elementi casuali):
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{Picture12.jpg}
    \label{etichetta34}
\end{figure}
\noindent Questo può essere utile perchè da qui possiamo vedere se c'è qualche classe strana, noi ad occhio essendo digits li riconosciamo subito, però risulta subito palese come l'1 sia 
un elemento più semplice del 3 o dell'8.

\subsubsection{Training}
La parte di training è molto importante ed è ciò che succede sotto il lavoro di addestramento.
\begin{lstlisting}
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
\end{lstlisting}
$\Uparrow$ Quando si definisce un meccanismo di addestramento di reti neurali si fanno 3 cose:
\begin{enumerate}
    \item Si definisce il modello su cui vai a fare il lavoro.
    \item Si definisce il criterio, ovvero l'obbiettivo che il modello deve andare ad ottimizzare.
    \item Si attua il meccanismo di \textbf{discesa stocastica del gradiente}, ovvero andiamo a computare con la nostra architettura la predizione che andiamo a fare sul nostro dato, andando a vedere quanto questo dato è discrepante dal risultato finale.
\end{enumerate}
$\Downarrow$ Questo è un ciclo di training di una rete neurale (in particolare sui dati che abbiamo visto prima)
\begin{lstlisting}[language=Python, basicstyle=\ttfamily\footnotesize, breaklines=true, frame=single]
# Train the model on CPU
start = time. time()    #calcolo il tempo
for epoch in tqdm(range(5)):    #5 iterazioni, un'epoca e' un intero giro di addestramento sul dataset (1 volta)
    model.train()   #metto il modello in modalita' di training
    running_loss = 0.0
    for images, labels in train_loader: #itero sul dataset composto da immagini e categoria 
        optimizer.zero_grad()   #processo stocastico, deve resettare il gradiente 
        outputs = model (images)    #faccio l'inferenza, do alla rete neurale l'input e lui restituira un output (questo prende l'immagine e ritorna la classe)
        loss = criterion(outputs, labels)   
        loss.backward()     #fa imparare il modello: prendo il modello, gli do un immagine e lui ci da una risposta. Confronto l'immagine col mio dato di riferimento, vedo quanto e' largo l'errore, piu' sbagli meglio impara
        optimizer.step()    #finito il giro ne fa un altro
        running_loss += loss.item()
    print(f'Epoch [{epoch +1}/5], Loss: {running_loss/len(train_loader) :. 4f}')
print(f'Training time: {time.time() - start :. 2f} seconds')
\end{lstlisting}
\vspace{2em}
Codice di valutazione del modello $\Downarrow$
\begin{lstlisting}[language=Python, basicstyle=\ttfamily\footnotesize, breaklines=true, frame=single]
# Evaluate the model
model.eval()    #non sono piu in training ma lo sto valutando
correct = 0     
total = 0
all_preds = []
all_labels = []

with torch.no_grad():    #il loop sara ora sul testing set    
    for images, labels in test_loader:  
        outputs = model(images) 
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
        all_preds.extend(predicted.numpy())
        all_labels.extend(labels.numpy())

accuracy = correct / total
print(f'nTest accuracy: {accuracy :. 4f}')  
\end{lstlisting}
$\Uparrow$ Questo blocco di codice mette il modello in modalità di valutazione (disabilitando dropout e batch‐norm adattativi) e poi, senza calcolare gradienti, scorre tutto il set di test generando predizioni per ogni batch. Confronta le etichette previste con quelle vere per contare quante sono corrette, tiene traccia del numero totale di esempi e accumula in due liste le predizioni e le etichette reali (utile per calcoli di metriche più avanzate). Infine calcola l’accuratezza come rapporto tra esempi classificati correttamente e totale degli esempi, e la stampa in output. 
\newpage
\noindent Fatto questo posso calcolarmi il classification report e la matrice di confusione .$\Downarrow$
\begin{lstlisting}
# Classification report
print("\nClassification Report:")
print(classification_report(all_labels, all_preds))

# Plot confusion matrix
conf_matrix = confusion_matrix(all_labels, all_preds)
plt.figure(figsize=(8,6))
sns. heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion)
plt.show()
\end{lstlisting}
$\Uparrow$ Questo pezzo di codice prende le liste delle etichette vere e delle predizioni raccolte durante la fase di valutazione e:
\begin{enumerate}
    \item Genera un report statistico: tramite \textbf{classification\_report} di scikit-learn, stampa per ciascuna classe metriche come precisione, richiamo (recall), F1-score e supporto (numero di esempi), offrendoti un quadro dettagliato delle prestazioni del modello su ogni categoria.
    \item Costruisce e visualizza la matrice di confusione: calcola con \textbf{confusion\_matrix} la tabella in cui le righe sono le etichette vere e le colonne quelle predette, quindi la disegna usando Seaborn. Il grafico mostra, cella per cella, il numero di occorrenze di ciascuna combinazione (vero positivo, falso negativo ecc.), con annotazioni numeriche e una scala di colori (“Blues”) che aiuta a individuare rapidamente dove il modello sbaglia di più.
    \item Etichettatura e rendering: assegna i nomi agli assi (“Predicted” e “True”), imposta un titolo (“Confusion”) e infine visualizza il grafico a schermo con \textbf{plt.show()}. In questo modo ottieni sia un sommario testuale delle prestazioni sia un’analisi visiva degli errori del tuo classificatore.
\end{enumerate}
\begin{center}
    Questa è la classification Report:
\end{center}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{Picture10.jpg}
    \label{etichetta35}
\end{figure}
\begin{center}
    Questa è la matrice di confusione:
\end{center}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{Picture14.jpg}
    \label{etichetta36}
\end{figure}
\noindent Notiamo ad esempio che l'8 è una classe difficile da interpretare, mentre l'1 una delle più facili.
\subsection{Come includere PyTorch and EDA nel nostro progetto con DevOps}
Partiamo da questo script:
\begin{lstlisting}[language=Python, basicstyle=\ttfamily\footnotesize, breaklines=true, frame=single]
" " "
Install requirements with :
pip install torch torchvision scikit - learn matplotlib seaborn tqdm
" " "
import time
import torch
import torch . nn as nn
import torchvision
import torchvision . transforms as transforms
from tqdm import tqdm
import numpy as np
import matplotlib . pyplot as plt
import seaborn as sns
from sklearn . metrics import classification_report , confusion_matrix
# Automatically download and load MNIST data
transform = transforms . Compose ([ transforms . ToTensor () ])
train_set = torchvision . datasets . MNIST ( root = ' ./ data ' , train = True ,
download = True , transform = transform )
test_set = torchvision . datasets . MNIST ( root = ' ./ data ' , train = False ,
download = True , transform = transform )
train_loader = torch . utils . data . DataLoader ( train_set , batch_size
=64 , shuffle = True )
test_loader = torch . utils . data . DataLoader ( test_set , batch_size
=1000 , shuffle = False )
\end{lstlisting}
\vspace{2em}
Aggiorniamo il requirements.txt:
\begin{itemize}
    \item Ricordiamoci di aggiungere tutti i nuovi pacchetti visti.
    \item Potremmo anche pensare di avere 2 diversi requirements, per esempio in produzione, dell' lint, del test e della coverage non ce ne facciamo niente.
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.4\textwidth]{Picture15.jpg}
        \label{etichetta37}
    \end{figure}
\end{itemize}
\vspace{2em}
Iniziamo a creare i file:
\begin{enumerate}
    \item Un file per l'EDA sul nostro dataset.
    \item Uno per il training separatamente.
\end{enumerate}
\newpage
La nostra cartella del progetto sarà strutturata in questo modo 
\begin{itemize}
    \item Un file per l'EDA (Eda.py)
    \item Un file per il training (Train\_mnist.py)
    \item Se non ignorati i dati verranno aggiunti, ricordiamoci quindi di cancellari nel gitignore, non li vogliamo nel source control.
\end{itemize}
\begin{center}
    $\Downarrow$
\end{center}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{Picture16.jpg}
    \label{etichetta38}
\end{figure}
\begin{lstlisting}
#GITIGNORE FILE (.gitignore)
*.pyc
__pycache__/
*.pyo 
*.pyd 

data/ 
*.pth 
\end{lstlisting}
Ci ritroveremo con questa situazione: 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{Picture17.jpg}
    \label{etichetta39}
\end{figure}
\noindent Vediamo la cartella data grigia ora quindi non inclusa. Escludiamo anche i file .pth, sono l'estensione dei dati della rete neurale addestrata. 
\vspace{2em}
Cerchiamo ora di riorganizzare il tutto:
\begin{itemize}
    \item creiamo una cartella per i test chiamata \textbf{tests}
    \item creiamo una cartella per gli script chiamata \textbf{src}
    \item lasciamo fuori solo:
    \begin{itemize}
        \item requirements
        \item MakeFile
        \item .gitignore
    \end{itemize}
\end{itemize}
A volte python non è capace di risolversi i path dei vari file, dobbiamo quindi modificare questa cosa:
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{Picture18.jpg}
    \label{etichetta40}
\end{figure}
\noindent Facciamo ora un po di codice per il train\_mnist :
\begin{lstlisting}[language=Python, basicstyle=\ttfamily\footnotesize, breaklines=true, frame=single]
class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        self.conv_layers = nn.Sequential(
            nn.Conv2d(1, 32, kernel_size=3),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(32, 64, kernel_size=3),
            nn.ReLU(),
            nn.MaxPool2d(2)
        )
        self.fc_layers = nn.Sequential(
            nn.Flatten(),
            nn.Linear(64 * 5 * 5, 64),  # output size after conv layers is [64,5,5]
            nn.ReLU(),
            nn.Linear(64, 10)
        )

    def forward(self, x):
        x = self.conv_layers(x)
        x = self.fc_layers(x)
        return x

def train_mnist(epochs: int, save_path: str = "mnist_model.pth"):
def evaluate_mnist(model_path: str = "mnist_model.pth"):

if __name__ == "__main__":
    train_mnist(5)
    evaluate_mnist()
\end{lstlisting}
$\Uparrow$ Questo codice definisce una rete neurale convoluzionale chiamata \textbf{SimpleCNN} utilizzando PyTorch, con due blocchi convoluzionali seguiti da due strati completamente connessi. La rete è progettata per classificare immagini in 10 classi, come quelle del dataset MNIST. Sono presenti due funzioni \textbf{train\_mnist} e \textbf{evaluate\_mnist}, ma non sono implementate. Nella sezione \textbf{if \_\_name\_\_ == "\_\_main\_\_"}, viene chiamata \textbf{train\_mnist(5)} per addestrare la rete per 5 epoche, e successivamente \textbf{evaluate\_mnist()} per valutarla, utilizzando i percorsi di default per salvare e caricare il modello \textbf{(mnist\_model.pth)}. Tuttavia, il codice non può essere eseguito correttamente finché le due funzioni non vengono implementate.
\newline 
\newline
La parte di training è così strutturata: 
\begin{lstlisting}[language=Python, basicstyle=\ttfamily\footnotesize, breaklines=true, frame=single]
def train_mnist(epochs: int, save_path: str = "mnist_model.pth"):
    # Automatically download and load MNIST data
    transform = transforms.Compose([transforms.ToTensor() ])
    train_set = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)

    train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)

    model = SimpleCNN()
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    # Train the model on CPU
    start = time. time()
    for epoch in tqdm(range(epochs)):
        model.train()
        running_loss = 0.0
        for images, labels in train_loader:
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
    print(f'Epoch [{epoch +1}/5], Loss: {running_loss/len(train_loader) :. 4f}')
print(f'Training time: {time.time() - start :. 2f} seconds')

torch. save(model.state_dict(), save_path)
\end{lstlisting}
$\Uparrow$ La funzione train\_mnist addestra il modello SimpleCNN sul 
dataset MNIST per un numero di epoche specificato. 
Il dataset viene scaricato automaticamente, 
trasformato in tensori e caricato in batch da 64. 
Viene istanziato il modello, definita la funzione di perdita e 
l'ottimizzatore Adam. Per ogni epoca, il modello viene messo in modalità training e addestrato su tutti i batch. Dopo ogni epoca viene stampata la loss media. Alla fine del training, viene stampato il tempo totale impiegato e salvato lo stato dei pesi del modello nel percorso specificato.
\newline
\newline 
La parte dell'evaluate\_mnist è così:
\begin{lstlisting}[language=Python, basicstyle=\ttfamily\footnotesize, breaklines=true, frame=single]
def evaluate_mnist(model_path: str = "mnist_model.pth"):
    # Automatically download and load MNIST data
    transform = transforms.Compose([transforms.ToTensor()])
    test_set = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)
    test_loader = torch.utils.data.DataLoader(test_set, batch_size=1000, shuffle=False)

    assert os.path.exists(model_path), f"Model file {model_path} does not exist."

    model = SimpleCNN()
    model.load_state_dict(torch.load(model_path))

    # Evaluate the model
    model.eval()
    correct = 0
    total = 0
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for images, labels in test_loader:
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
            all_preds.extend(predicted.numpy())
            all_labels.extend(labels.numpy())

    accuracy = correct / total
    print(f'\nTest accuracy: {accuracy:.4f}')

    # Classification report
    print("\nClassification Report:")
    print(classification_report(all_labels, all_preds))

    # Plot confusion matrix
    conf_matrix = confusion_matrix(all_labels, all_preds)
    plt.figure(figsize=(8,6))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show(block=False)
    plt.savefig("confusion_matrix.png")
\end{lstlisting}
$\Uparrow$ Il codice serve a valutare un modello di rete neurale convoluzionale (SimpleCNN) già addestrato sul dataset MNIST, che contiene immagini di cifre scritte a mano. Una volta caricato il modello da file, il codice:
\begin{enumerate}
    \item Scarica e prepara automaticamente il set di test MNIST, normalizzando le immagini.
    \item Esegue la valutazione del modello in modalità "no\_grad" (senza aggiornare i pesi), calcolando il numero di predizioni corrette sul test set.
    \item Stampa l'accuratezza del modello sul test set.
    \item Genera un classification report contenente precision, recall e F1\_score per ciascuna cifra (da 0 a 9).
    \item Crea e salva un'immagine della confusion matrix, che mostra come il modello ha classificato ogni cifra, facilitando l'analisi degli errori.
\end{enumerate}
In sintesi, il codice fornisce una valutazione completa delle performance del modello su dati mai visti prima.
\newline
\newline 
Detto questo, come facciamo a fare Continous Integration di tutto ciò che abbiamo visto? 
Per testarla dovremo fare il training di tutto, ma non si può fare ad ogni commit questa cosa, dobbiamo addestrare per ogni epoca.
\begin{lstlisting}[language=Python, basicstyle=\ttfamily\footnotesize, breaklines=true, frame=single]
    import os
from src.train_mnist import train_mnist, evaluate_mnist

def test_mnist_training():
    test_model_path = "test_mnist_model.pth"
    # Train for 1 epoch
    train_mnist(1, save_path=test_model_path)
    # Check if the model file is created
    assert os.path.exists(test_model_path), "Model file not found after training."
    # Check if the model file is not empty, and can be loaded
    assert os.path.getsize(test_model_path) > 0, "Model file is empty."
    # Load the model to ensure it can be loaded without errors
    import torch
    from src.train_mnist import SimpleCNN
    model = SimpleCNN()
    model.load_state_dict(torch.load(test_model_path))

    # Do a simple inference to check if the model is working
    xin = torch.randn(1, 1, 28, 28)  # Random input tensor
    model.eval()
    with torch.no_grad():
        output = model(xin)
    assert output.shape == (1, 10), "Model output shape is incorrect."
    # Clean up the model file after the test

    os.remove(test_model_path)
    os.remove("confusion_matrix.png")
    print("Test passed: Model trained and saved successfully.")

def test_mnist_evaluation():
    # Train the model first
    test_model_path = "test_mnist_model.pth"
    train_mnist(1, save_path=test_model_path)
    evaluate_mnist(test_model_path)
    os.remove(test_model_path)
\end{lstlisting}
Il codice contiene due funzioni di test che servono per verificare che il processo di training e valutazione di un modello su MNIST funzioni correttamente.
\begin{enumerate}
    \item  test\_mnist\_training():
    Questa funzione testa che l'addestramento e il salvataggio del modello funzionino come previsto:
    \begin{itemize}
        \item Allena il modello per una sola epoca e lo salva su file.
        \item Verifica che il file del modello esista e che non sia vuoto.
        \item Prova a caricare il modello salvato per assicurarsi che sia leggibile.
        \item Esegue una inferenza con un input casuale (simulando un'immagine MNIST), per controllare che la forma dell'output sia corretta (vettore di 10 classi, una per cifra da 0 a 9).
        \item Elimina i file generati (.pth e immagine della confusion matrix) per pulire l'ambiente di test.
        \item Stampa un messaggio di successo.
    \end{itemize}
    \item test\_mnist\_evaluation()
    Questa funzione serve a testare la valutazione del modello:
    \begin{itemize}
        \item Addestra nuovamente il modello per una sola epoca.
        \item Valuta il modello usando la funzione evaluate\_mnist, che calcola accuratezza, classification report e confusion matrix.
        \item Alla fine, rimuove il file del modello.
    \end{itemize}
    \item L' obbiettivo complessivo è assicurarsi che:
    \begin{itemize}
        \item Il modello possa essere addestrato, salvato e ricaricato senza errori.
        \item L'inferenza funzioni come previsto.
        \item Il processo di valutazione non generi eccezioni o comportamenti anomali.
    \end{itemize}
\end{enumerate}
L'EDA (codice visto primo di creazione dei plot \dots), è stato messo dentro un file chiamato eda, dentro una cartella chiamata eda. Quando faccio l'eda semplicemente lo eseguo, vedo che mi vengano creati i file e poi li cancello. Se tutto funziona dovrenne essere apposto.
\begin{lstlisting}[language=Python, basicstyle=\ttfamily\footnotesize, breaklines=true, frame=single]
import os
from src.eda import eda

def test_eda():
    eda()  # Run the EDA function to check if it executes without errors
    os.remove("class_distribution.png")
    os.remove("sample_images.png")
    print("EDA test passed: EDA function executed successfully.")
\end{lstlisting}
Andiamo ad aggiornare anche il MakeFile, rimane praticamente uguale:
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Picture19.jpg}
    \label{etichetta41}
\end{figure}
\noindent Alla fine eseguo, faccio \framebox{make test} ed ottengo questi risultati:
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Picture20.jpg}
    \label{etichetta42}
\end{figure}
\noindent Una cosa importante è la coverage, ovvero il controllo che tutte le righe del mio script siano state eseguite. 

\subsection{Pytorch Lightning}
\noindent \'{E} una libreria che ottimizzano il lavoro di addestraento delle reti neurali, ad esempio, nel codice che vediamo $\Downarrow$, gli diamo il modello in pasto e sarà lui che si occuperà di fare il training del modello.
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Picture21.jpg}
    \label{etichetta43}
\end{figure}
Questo codice implementa un esempio minimale di training utilizzando il framework PyTorch Lightning, con lo scopo di dimostrare la struttura base di un modello senza un obiettivo pratico specifico. Ecco cosa fa nel complesso:
\begin{enumerate}
    \item Definizione del modello:
    \begin{itemize}
        \item Viene creato un modello di machine learning semplice: una rete neurale con un singolo strato lineare (nn.Linear)
        \item Il modello prende in input 32 valori e restituisce 2 valori in output.
    \end{itemize}
    \item Setup del training: 
    \begin{itemize}
        \item Dati casuali: I dati di training sono generati casualmente (tensori di forma 8x32), simulando un dataset con 8 esempi e 32 feature ciascuno.
        \item Ottimizzatore: Viene configurato l'ottimizzatore Adam per aggiornare i parametri del modello.
    \end{itemize}
    \item Logica di training:
    \begin{itemize}
        \item Il modello elabora un batch di dati.
        \item La loss è calcolata come somma di tutti i valori in output del modello (invece di una loss tradizionale come MSE o CrossEntropy).
        \item Esempio: se l'output è [[0.1, -0.2], [0.3, 0.4]], la loss sarà 0.1 - 0.2 + 0.3 + 0.4 = 0.6.
        \item Obbiettivo: Minimizzare questa loss arbitraria (senza un significato statistico reale).
    \end{itemize}
    \item Esecuzione: 
    \begin{itemize}
        \item Il codice allena il modello per 1 epoch (poiché il dataloader ha un solo batch di 8 esempi).
        \item Usa il Trainer di PyTorch Lightning, che gestisce automaticamente:
        \begin{itemize}
            \item L'aggiornamento dei parametri.
            \item La distribuzione su hardware (CPU/GPU se disponibile).
            \item Il logging (non presente in questo esempio).
        \end{itemize}
    \end{itemize}
\end{enumerate}

\subsection{Come gestiamo i modelli che già esistono?}
Esistono diversi siti in cui ci sono modelli già addestrati, che ci danno accesso a modelli con dataset o approcci già settati e preaddestrati. 
Andrò quindi, in base alle mie esigenze, a scegliere i modelli dai vari gruppi di ricerca e dalle varie repository. Per il progetto si consiglia il sito Hugging Face.
\section{iperparametri}
Sono una serie di parametri che non controllano di per se il comportamento del modello, bensì ne controllano la capcità e le performance durante il training.
Abbiamo ad esempio:
\begin{itemize}
    \item Learning rate: quanto rapidamente il modello deve imparare, più è alto più inizialmente imparerà in fretta, se è troppo alto dopo un tot non riuscirà a diventare più bravo.
    \item Dropout: valori casuali che vengono fatti durante le operazioni di training.
    \item Tipo di ottimizzatore. 
    \item Weight decay.
    \item Activation functions. 
    \item Scheduler parameters.
\end{itemize}
Tutti questi iperparametri, sono dei processi che richiedono un tuning manuale ed è molto time-consuming. Ci sono dei parametri che con ricerca a tappeto (grid search) puoi andare ad esplorare e vedere quali sono quelli ottimi per il mio sistema. 
\subsection{Grid Search: Idea} 
\begin{itemize}
    \item Decido una griglia di iperparametri.
    \item Facciamo training e testing di tutti questi valori.
    \item Seleziono quelli che funzionano meglio.
\end{itemize}
Per fare un buon lavoro dobbiamo quindi decidere il mio modello, decidere quali sono gli iperparametri da andare ad ottimizzare, facciamo dei cicli di loop iterativi per lavorare su tutte le possibili combinazioni, li associo al mio training set e avrò i migliori parametri.
Con strumenti come Optuna si può fare tutto questo lavoro in automatico, farà un buon sampling dei vari parametri tramite suoi algoritmi interni. 
\begin{lstlisting}[language=Python, basicstyle=\ttfamily\footnotesize, breaklines=true, frame=single]
# optuna_search.py
import optuna
import torch
from torchvision import datasets, transforms

def objective(trial):
    # Hyperparameters to tune
    lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)
    dropout = trial.suggest_float('dropout', 0.1, 0.5)
    
    # Model definition
    model = torch.nn.Sequential(
        torch.nn.Flatten(),
        torch.nn.Linear(28*28, 128),
        torch.nn.ReLU(),
        torch.nn.Dropout(dropout),
        torch.nn.Linear(128, 10)
    )
    
    # Training loop (one epoch for demo)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    criterion = torch.nn.CrossEntropyLoss()
    train_loader = torch.utils.data.DataLoader(
        datasets.MNIST('.', train=True, download=True, transform=transforms.ToTensor()),
        batch_size=64, shuffle=True
    )
    
    model.train()
    for batch in train_loader:
        data, target = batch
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()

    # Validation accuracy as objective
    val_loader = torch.utils.data.DataLoader(
        datasets.MNIST('.', train=False, download=True, transform=transforms.ToTensor()),
        batch_size=1000, shuffle=False
    )
    correct = 0
    with torch.no_grad():
        for data, target in val_loader:
            preds = model(data).argmax(dim=1)
            correct += (preds == target).sum().item()
    accuracy = correct / len(val_loader.dataset)
    return accuracy

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=20)    
\end{lstlisting}
$\Uparrow$ Questo codice carica un modello già addestrato per il riconoscimento delle cifre scritte a mano (MNIST), lo valuta sul set di test calcolandone l'accuratezza, stampa un report con le metriche di classificazione e genera una confusion matrix per visualizzare gli errori di predizione.
\newline 
La cosa principale è decidere quindi quali sono i parametri da addestrare, il modello può essere qualsiasi cosa, i dati possono essere i nostri, facciamo il training e vogliamo massimizzare l'accuratezza.
\begin{lstlisting}[language=Python, basicstyle=\ttfamily\footnotesize, breaklines=true, frame=single]
# optuna_search.py
import optuna
import torch
from torchvision import datasets, transforms

def objective(trial):
    # Hyperparameters to tune
    lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)
    dropout = trial.suggest_float('dropout', 0.1, 0.5)
    
    # Model definition
    model = torch.nn.Sequential(
        torch.nn.Flatten(),
        torch.nn.Linear(28*28, 128),
        torch.nn.ReLU(),
        torch.nn.Dropout(dropout),
        torch.nn.Linear(128, 10)
    )
    
    # Training loop (one epoch for demo)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    criterion = torch.nn.CrossEntropyLoss()
    train_loader = torch.utils.data.DataLoader(
        datasets.MNIST('.', train=True, download=True, transform=transforms.ToTensor()),
        batch_size=64, shuffle=True
    )
    
    model.train()
    for batch in train_loader:
        data, target = batch
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()

    # Validation accuracy as objective
    val_loader = torch.utils.data.DataLoader(
        datasets.MNIST('.', train=False, download=True, transform=transforms.ToTensor()),
        batch_size=1000, shuffle=False
    )
    correct = 0
    with torch.no_grad():
        for data, target in val_loader:
            preds = model(data).argmax(dim=1)
            correct += (preds == target).sum().item()
    accuracy = correct / len(val_loader.dataset)
    return accuracy

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=20)
\end{lstlisting}
Ottenendo questi risultati: 
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Picture22.jpg}
    \label{etichetta44}
\end{figure}
\noindent Alla fine ci dirà che il migliore è quello che ha il Learning Rate di 0.00667... e dil dropout di 0.1
Questo vale di più per la parte sperimentale.

\section{Docker, Logging e Monitoring}
Docker è una tecnologia che permette di rilasciare software su delle macchine, andando a standardizzare l'installazione e l'esecuzione del codice. Togliamo così tutti i problemi relativi all'utilizzo del codice
su macchine diverse dalla nostra. Lato codice ci abbiamo già pensato un po' con DevOps. Con il Docker andremo quind a standardizzare la parte di esecuzione.
Guarderemo anche un po' il monitoring ed il logging, non li vedremo però applicati. 

\subsection{Docker}
Come detto prima abbiamo dei problemi da risolvere sul nostro software:
\begin{itemize}
    \item Identifico questo problema.
    \item Scrivo il codice. 
    \item Lo testo sulla mia macchina.
    \item Infine lo metto in produzione.
    Qui troveremo probabilmente degli errori quindi:
    \begin{itemize}
        \item Controlleremo che il nostro codice sia apposto.
        \item Lo verifico localmente.
        \item Faccio Deploy.
        \item E via dicendo.
    \end{itemize}
    Con molta probabilità ci ritroveremo comunque con degli errori su altre macchine.
\end{itemize}
Entra quindi in gioco Docker, ma cosa fa?
\begin{itemize}
    \item Crea i \textbf{Contaners}, sono sostanzialmente delle soluzioni leggere, portatili ed autoconenute. In pratica definiamo le nostre specifiche una sola volta e siamo capaci di esegure il nostro codice ovunque.
    \item Docker non risolve completamente il problema, va installato su tutte le macchine, una volta che tutti hanno l'installazione ed è standardizzata possiamo lavorare su più sistemi per l'appunto.  
\end{itemize}

\subsection{Containerization}
\'{E} una sorta di versione evoluta della virtualizzazione, non è virtualizzazione vera e propria, il vantaggio è che è facle replicarlo. \'{E} modulare, posso quindi avere più container, anche grandi, che comunicano tra di loro, garantendoci grande scalabilità.
\'{E} abbastanza rapido, ha un overhead abbastanza basso non avendo tutto il sistema operativo sotto, d'altra parte è vero che per le applcazioni di Machine Learning non è velocissimo. Questo succede perchè quando vado a toccare l'hardware, non è più solo docker che lavora ma anche la macchina.
\'{E} molto efficiente perchè ci permette di separare bene i microsevizi, ed è portatile, basta solo la configurazione (potendolo così personalizzare).
I concetti base sono dunque:
\begin{itemize}
    \item Container: pacchetto autocontenuto\dots
    \item Immagini: il nostro software crea una cosiddetta immagine ed un'istanza dell'immagine è un Container. Quindi se io ho un'immagine che rappresenta il mio programma e lancio 50 volte il mio programma, sto lanciando 50 container della mia immagine.
    \item Non c'è un sistema di virtualizzazione, quindi non c'è un overhead al lancio dell'applicativo, avremo quindi delle performance migliori.
    \item Gestisce da solo le risorse.
\end{itemize}
\subsubsection{Docker vs Virtual Machine}
I container \underline{non sono Virtual Machine}, sostanzialmente invece che avere il OS, i driver, i kernel\dots Nella VM avremo quindi un sacco di lavoro tra macchina e OS. Docker è un processo isolato, 
ha bisogno soltanto dei file per l'esecuzione. Possiamo mandare in esecuzione diversi container, ma condividono tutte lo stesso kernel, potendo avere un impatto infrastrutturale ridotto. Questa è una rappresentazione visiva di come è strutturato $\Downarrow$
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{Picture23.jpg}
    \label{etichetta45}
\end{figure}
\begin{center}
    Questa invece è la rappresenztazione con VM $\Downarrow$
\end{center}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{Picture24.jpg}
    \label{etichetta46}
\end{figure}
Vedia quindi la differenza, il OS in più è quello che appesantirà il lavoro, insieme agli Hypervisor.
I benefici sono soprattutto:
\begin{itemize}
    \item Veloce da svluppare, una volta che abbiamo l'applicazione fatta bastano poche righe di codice per farlo.
    \item Veloce da rilasciare, lanciare il container è una riga.
    \item Puoi fare molto bene il controllo sui servizi.
    \item \'{E} molto comodo grazie all'auto\-release.
\end{itemize}
Le piattaforme possiamo usare sia windows che Linux(originariamente è stato progettato per Linux), ora con WSL possiamo girarlo nativamente su windows in maniera più diretta.
C'é Docker Desktop, non è perfetto proprio perchè non gira su un sistema operativo Linux, non va molto bene per addestramento con GPU.

\subsubsection{Docker commands}
I comandi (su terminale) generali di Docker sono:
\begin{itemize}
    \item docker <main command><secondary command> [parameters]
    \item scrivendo docker vedremo i comandi disponibili
    \item per la versione basterà scrivere docker version o docker -v
\end{itemize}

\subsection{Immagine}
Sono una serie di file che vengono cumulativamente messi uno sull'altro formando un punto di partenza del nostro codice, in pratica prendo il codice da zero, installo tutti i requirements, fatto questo diventa un'immagine. Generalmente si prende un'immagine pre-costruita per poi personalizzarla.
Ogni cambiamento a queste immagini diventa un layer, questo layer ha un identificativo univoco e viene salvato sulla macchina dove stiamo compilando Docker. In pratica dal nostro punto di partenza, se vogliamo installare i nostri requisiti lo diciamo a Docker e lui prenderà l'immagine originale, che diventerà un layer di partenza,
creando alla fine un'immagine composta dal layer di partenza e quello che abbiamo modificato ottenendo così un'immagine.
Ogni layer pesa, quindi bisogna cercare di creare meno layer possibili.
\newline
Alcuni comandi sono:
\begin{itemize}
    \item Per vedere le immagini in cache locale: docker image ls
    \item Per scaricare un'immagine dal docker hub: docker pull <nomeImmagine>
    \item Per cancellare un'immagine: docker image rm <nomeImmagine>
    \item Per rimuovere le immagini non usate: docker image prune
\end{itemize}
Dove vengono mandate le immagini? Le immagini possono essere salavte in locale, se voglio però distribuirli su altri computer abbiamo Docker Hub. \'{E} un contenitore condiviso in cui possimao condividere le nostre immagini e prenderne altre già fatte.
Da qui ci basterà fare \framebox{docker pull hello-world}, e lui ci scaricherà l'immagine, se non la trova lui ci scaricherà l'ultima tramite il tag latest.
La pulla da library/Hello-World, una volta scaricata per caricare l'immagine ci bastera scrivere \framebox{docker run nome-immagine}.
Ogni volta che eseguiamo un container gli diamo un nome temporaneo, viene eseguito il codice ed una volta eseguito il container muore.
\newline
Abbiamo quindi altri comandi utili:
\begin{itemize}
    \item Per assegnare un taggare un'immagine: docker image tag <nomeImmagine>[:TAG] <nomeImmagine>[:TAG]
    \item Per vedere la storia di un'immagine: docker image history <nomeImmagine>
    \item Per vedere il contenuto/configurazione di un'immagine: docker image insepct <nomeImmagine>
\end{itemize}
Il container come dicevamo è l'istanza di un'immagine ed inizialmente non è altro che una copia, quello che succede è che l'immagine pian piano evolve, ad esempio vengono creati dei file temporanei,
viene salvato qualcosa sul disco\dots Fa dei cambiamenti nel container ma non nell'immagine, infatti se facciamo modifiche al container durante l'esecuzione, quando muore il container perdiamo tutti i file. 
Non si salva quindi mai nulla nel container, a meno che non sappiamo come recuperarlo, ad esempio tramite i \textbf{Volumi}. 
\newline
Comandi un po' più operativi:
\begin{itemize}
    \item Per vedere i container attivi: docker ps 
    \item Per vedere tutti quelli sulla macchina: docker ps -a
\end{itemize}
\subsubsection{Docker Run}
docker run [-v,-d.-p.options]<nomeImmagine>
\begin{itemize}
    \item -p <portaHost>:<portaContainer> = espone una porta $\rightarrow$ Ogni container viene eseguito e comunica su uno stato di rete isolato, per farlo parlare con l'esterno si lancia questo comando. Questo comando espone una porta di Docker con una porta dell'interfaccia di rete della macchina su cui stiamo eseguendo Docker (è un ponte).
    \item -d = deteached mode (resta attivo in background) $\rightarrow$ Fa semplicemente partire il processo in background lasciando al sistema operativo la gestione.
    \item -v <nomeVolume>:<percorsoNelContainer> = Assegna un volume definito $\rightarrow$ \'{E} il percorso assoluto nella macchina, contro il percorso che avrà poi nel container (lo vedremo tra poco).
    \item -rm =non lascia traccia del container al termine $\rightarrow$ \'{E} utile se vogliamo eliminare l'istanza del container, utile quando il container deve nascere e morire senza popolare la lista.
    \item docker run -it <nomeImmagine> bash $\rightarrow$ per avviare un'mmagine ed entrare nel suo terminale, possiamo entrare in quel container come se fossimo all'interno di quella macchina (utile per il debug).
    \item docker container exec -it <nomeContainer> <bash|sh> $\rightarrow$ se vogliamo entrare nella shell di un container già in esecuzione.
    \item docker container port <nomeContainer> $\rightarrow$ per vedere le porte condivise tra host e container.
\end{itemize}

\subsubsection{DOckerfile}
\'{E} un semplice file di testo senza estensione (la si aggiunge solo se vogliamo fare versioni diverse). Ci serve un'immagine da cui partire ed ogni nuova riga che aggiungiamo è un nuovo layer, l'insieme di tutti quanti i layer è un'immagine. 
Più layer utilizziamo più spazio utilizziamo, quindi più modifiche verranno fatte durante la nostra build.
\newline
Comandi importanti del dockerfile:
\begin{itemize}
    \item \textbf{ADD: }ci permettere di aggiungere dati direttamente dal sistema operativo corrente al container.
    \item \textbf{ARG: }se vogliamo specificare delle variabili che vengono messe durante la fase di compilazione.
    \item \textbf{CMD: }specifica il comando da eseguire alla fine dell'esecuzione del container.
    \item \textbf{COPY: }come l'add ma qui puoi mantenere la struttura delle cartelle.
    \item \textbf{ENTRYPOINT: }ci dice qual'è l' eseguibile di partenza del nostro container.
    \item \textbf{ENV: }per descrivere delle variabili d'ambiente.
    \item \textbf{EXPOSE: }per dire quali porte sono aperte.
    \item \textbf{FROM: }crea una nuova build per un'immagine di partenza.
    \item \textbf{HEALTHCHECK: }per fare un check prima della partenza del container.
    \item \textbf{LABEL: }se vogliamo aggiungere dei metadati.
    \item \textbf{MAINTEINER: }specifica l'autore dell'immagine.
    \item \textbf{RUN: }ogni run crea un nuovo layer.
    \item \textbf{WORKDIR: }ci permette di dire in quale cartella tutte le operazioni devono essere eseguite.
\end{itemize}
\newpage
Ecco un esempio:
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Picture25.jpg}
    \label{etichetta47}
\end{figure}
\noindent Qui è stato creato uno script con un ciclo infinito, prende il tempo corrente e lo formatta in ore, minuti, secondi e anno, mese, giorno; ogni secondo stampa.
Ed infine il DockerFile (ogni riga crea un nuovo layer): 
\begin{lstlisting}[language=Python, basicstyle=\ttfamily\footnotesize, breaklines=true, frame=single]
from python:3.11-slim

#set the working directory 
WORKDIR /app  #viene creata automaticamente una cartella se non esiste
#copy the requirements file into the container 
COPY requirements.txt  #copio il file in . ovvero la cartella corrente 
#install the dependecies
RUN pip install --no-cache-dir -r requirements.txt  #istruzione che mi permette di eseguire un comando come fosse una cmd nnormale
#copy the rest of the application code into the container 
COPY . .    #copia tutti file che ci sono nella cartella all'interno della cartella di destinazione

#run python script at container startup 
#-u flag is used to force the stdout and stderr streams to be unbuffered, wich can be useful for real-time logging 
CMD ["python", "-u", "script.py"]  
\end{lstlisting}
$\Uparrow$ Questo Dockerfile crea un'immagine Docker per eseguire uno script Python, seguendo una struttura ottimizzata:
\begin{enumerate}
    \item Base Image: Parte da un'immagine ufficiale leggera di Python 3.11 (python:3.11-slim), ideale per applicazioni minimaliste.
    \item Configurazione ambiente: Imposta la directory di lavoro /app (creata automaticamente se non esiste).
    \item Gestione dipendenze: 
    \begin{itemize}
        \item Copia solo il file requirements.txt nella directory di lavoro
        \item Installa le librerie Python specificate senza memorizzare cache, riducendo le dimensioni finali dell'immagine.
    \end{itemize}
    \item Deploy codice: Dopo l'installazione delle dipendenze, copia tutto il codice sorgente rimanente (file e cartelle locali) nella directory /app.
    \item Esecuzione automatica: All'avvio del container, esegue automaticamente: python -u script.py. Il flag -u garantisce un output immediato dei log (senza buffering), cruciale per monitoraggio in tempo reale.
\end{enumerate}
In pratica costruisce un ambiente containerizzato pronto a eseguire lo script script.py con tutte le sue dipendenze, ottimizzato per dimensioni ridotte e logging efficiente.
Per fare il building devo scrivere:
\newline
docker build -t<imageName> o docker build -t<imageName> -f./Dockerfile 
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Picture26.jpg}
    \label{etichetta48}
\end{figure}
\newpage
\noindent Fatto ciò facciamo un test con \framebox{docker run test}.
\newline
Se voglio lanciarlo in background posso fare \framebox{docker run -d test}.
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Picture27.jpg}
    \label{etichetta49}
\end{figure}
\noindent Il comando docker log test serve per vedere cosa sta loggando, naturalmente qui non troverà nulla, perchè quando ho lanciato docker run gli abbiamo assegnato un nome, quindi dobbiamo andare a cercare quel nome. Con \framebox{docker ps} troveremo quello che cerchiamo, in questo caso lucid\_panini. Vedremo poi le informazioni.
Facendo \framebox{docker log --follow "nome"}, seguirà il trace dello standard output del container fino ad un nostro interrupt da tastiera (ctrl+c). Facendo invece \framebox{docker stop "nome"}, fermeremo il container. Possiamo dare un nome al docker con \framebox{docer run -d --name "nome" test}.

\subsection{Volumi}
I volumi sono molto importanti perchè ci permettono di passare i dati da dentro a fuori i conatiner e viceversa. Abbiamo essenzialmente una sorta di cartella condivisa, in cui noi facciamo partire il container, modifichiamo dei file all'interno del volume,
fermiamo il container e abbiamo ancora i nostri dati in mano.
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Picture28.jpg}
    \label{etichetta50}
\end{figure}
\noindent Come si fa questa cosa ? 
\newline
Facciamo un esempio:
\begin{itemize}
    \item Cambio lo script chiamato create\_file\_with\_content.
    \begin{figure}[H]
        \centering
        \includegraphics[width=1\textwidth]{Picture29.jpg}
    \label{etichetta51}
\end{figure}
    \item Apro un file .txt e ci scrivo sopra del contenuto.
    \item Cambio poi il comando perchè avendo cambiato lo script voglio fare andare il nuovo.
    \begin{figure}[H]
        \centering
        \includegraphics[width=1\textwidth]{Picture30.jpg}
        \label{etichetta52}
    \end{figure}
    \item Buildo il dockerfile.
    \begin{figure}[H]
        \centering
        \includegraphics[width=1\textwidth]{Picture31.jpg}
        \label{etichetta53}
    \end{figure}
\end{itemize}
Quando arrivo in fondo vado a prendere il mio docker file modificato:
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Picture26.png}
\end{figure}
\noindent Qui vediamo che l'unica modifica è il command in fondo. E andiamo a fare: 
\newline
\framebox{docker run -v 'pwd':/app test} (questo è un comando per linux, in windows dobbiamo specificare il percorso assoluto). Con questo comadno mappiamo la cartella docker tests con la cartella app, facciamo mapping dell'intera cartella quindi 
l'eliminazione della cartella eqiuvale alla cancellazione completa dal disco.
A questo punto avvio il \framebox{docker run -v 'pwd' :/app test} ed il file verrà fisicamente buttato sul sistema operativo (non dobbiamo fare mai l'intero mapping dell'intera cartella, generalmente si mappano cartelle vuote o specifiche).
\subsection{Integrazione con VSCode del docker}
Una volta sistemato tutto per farci sviluppo dobbiamo installare un'estensionde per docker chiamata per l'appunto Docker.
Fatto ciò ci apparirà un'icona nella barra laterale, facciamo ora un piccolo training:
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Picture27.png}
\end{figure}
Questo codice è un semplice training fatto da ultralytics (package preso da Hugging Face), è un esempio diverso di training, questa volta abbiamo un pacchetto che ci permette di astrarre un pochino la parte di training di questa architettura chiamata YOLO.
Detto questo parliamo di come funziona il tutto:
\begin{itemize}
    \item Creo il codice.
    \item Creo il mio container.
    \item Quando lo voglio far partire schiaccio start:
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.3\textwidth]{Picture28.png}
    \end{figure}
    Da qui vedo in tempo reale tutto il contenuto del container.
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.3\textwidth]{Picture29.png}
    \end{figure}
\end{itemize}
Abbiamo quindi risolto il problema ? \textbf{NO}, ci sono ancora differenze tra le versioni e le piattaforme, ci stiamo però avvicinando all'automazione delle cose. Il problema si verifica quando dobbiamo lavorare con la gpu o le reti.

\subsection{Allenare un modello in Docker}
In precedenza abbiamo visto il training tradizionale, andremo ora a trattare quello più ad alto livello, generalmente pù semplice da capire per i novizi (sotto però fa le stesse identiche cose del training tradizionale). 
Facciamo un esempio con l'object detection, ovvere modello che ci dice quanti oggetti, dove sono e quanto spazio occupano nell'immagine. Ecco una foto di esempio:
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Picture30.png}
\end{figure}
\noindent Come esempio di dataset utilizzeremo COCO, dataset con diverse foto di migliaia di cose diverse. Un altro esempio come detto prima è YOLO, modello popolare di identificazione di oggetti. 
A noi interessa com'è strutturata la rete neurale, e questo è un piccolo schema:
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Picture31.png}
\end{figure}
\noindent Aggiorniamo ora i requirementes, installando ultralytics, e questo è il codice per il training (riga 13-22) $\downarrow$:
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Picture32.png}
\end{figure}
Come funziona questo codice?
\begin{itemize}
    \item Gli diamo la versione di YOLO che vogliamo utilizzare.
    \item Gli diciamo che vogliamo addestrare su coco128.
    \item Addestro per un certo numero di epoche.
    \item Come device usiamo la cpu, (anche per il progetto useremo la cpu per una questione di macchina, il nostro compito per il progetto sarà capire come si fa il processo e la pipeline).
    \item Infine L'output tensor\_path è il modello addestrato, salvato in una cartella chiamata results.
    \item Carico il modello.
    \item Salvo con torch.save
\end{itemize}
Andiamo poi a modificare anche il dockerfile:
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Picture33.png}
\end{figure}
\noindent Abbiamo aggiunto una riga (riga 3), metto in coda per avere un solo layer finale. 
Poi buildo dandogli un nome ed infine facciamo il training. 
\newline
Ora dobbiamo fare:
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Picture34.png}
\end{figure}
\noindent Questo perchè vogliamo che il results che salvo dentro al container finisca fuori. Infine lanciamo:
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Picture35.png}
\end{figure}
\noindent Quando finisce vedremo le performance: 
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Picture36.png}
\end{figure}
Quando finisce tutto vado a cercare il mio dato con il comando \framebox{docker ps -a}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\textwidth]{Picture37.png}
\end{figure}

\subsection{Inferenza}
L'inferenza è il processo mediante il quale un modello di machine learning già addestrato viene utilizzato per fare previsioni o classificazioni su nuovi dati mai visti prima. In pratica, dopo aver completato la fase di training e aver salvato i pesi del modello, si carica il modello e si forniscono in input nuovi dati per ottenere un output predittivo.
\newline
Nel contesto di un modello di deep learning (ad esempio una rete neurale PyTorch), il flusso tipico di inferenza è:

\begin{enumerate}
    \item \textbf{Caricamento del modello addestrato:} si caricano i pesi salvati tramite \texttt{model.load\_state\_dict()}.
    \item \textbf{Preparazione dei dati di input:} i dati devono essere preprocessati nello stesso modo usato durante il training (es. normalizzazione, ridimensionamento, conversione in tensori).
    \item \textbf{Impostazione del modello in modalità valutazione:} si usa \texttt{model.eval()} per disabilitare comportamenti specifici del training come dropout e batch normalization adattativa.
    \item \textbf{Esecuzione della predizione:} si passa il dato al modello e si ottiene l'output, tipicamente all'interno di un blocco \texttt{with torch.no\_grad():} per evitare il calcolo dei gradienti.
\end{enumerate}

\textbf{Esempio pratico in PyTorch:}
\begin{lstlisting}[language=Python, basicstyle=\ttfamily\footnotesize, breaklines=true, frame=single]
import torch
from src.train_mnist import SimpleCNN

# Carica il modello addestrato
model = SimpleCNN()
model.load_state_dict(torch.load("mnist_model.pth"))
model.eval()

# Prepara un'immagine di input (ad esempio una singola immagine MNIST)
# Supponiamo che 'image' sia un tensore di forma [1, 28, 28]
image = ...  # Carica e preprocessa l'immagine

# Aggiungi la dimensione batch e passa in float32
input_tensor = image.unsqueeze(0).float()

# Esegui l'inferenza
with torch.no_grad():
    output = model(input_tensor)
    predicted_class = output.argmax(dim=1).item()

print(f"Classe predetta: {predicted_class}")
\end{lstlisting}

\textbf{Nota:} In produzione, l'inferenza viene spesso eseguita in ambienti containerizzati (ad esempio tramite Docker) per garantire portabilità e riproducibilità del risultato.

\textbf{In sintesi:} l'inferenza è la fase in cui il modello, già addestrato, viene utilizzato per generare previsioni su nuovi dati, rappresentando il vero valore applicativo del machine learning.

\subsection{CI/CD con Docker}
Ovviamente si può integrare Docker nel nostro CD/CI, tendenzialmente sono molto pesanti quindi come detto prima si consiglia di partire da cose piccole. Questo è un esempio di CI/CD con Docker:
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Picture40.png}
\end{figure}
\newpage

\section{Logging e monitoring}
Nel logging ci sono 3 livelli:
\begin{enumerate}
    \item Livello sperimentale.
    \item Livello strutturale.
    \item Livello log aggregation.
\end{enumerate}
A livello di training ci sono diversi modi di loggare le informazioni, Wights \& Biases è un ottimo strumento per questo.
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Picture41.png}
\end{figure}
\noindent Questa è la parte di training, come vediamo ogni tanto viene fatto un log, con wandb, una volta inizializzato un progetto e fatto tutti i cicli ci stampa, invece che sulla riga di comando diversi grafici tramite interfaccia grafica.
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Picture42.png}
\end{figure}
\noindent Naturalmente possiamo decidere noi cosa loggare. Possiamo farlo anche con le matrici di confusione. 

\subsection{Log aggregation and training}
Ci permette di centralizzare gli standard output ma anche gli errori, permettendo di avere un log anche quando sono distribuite le macchine. Con questa cosa qui, se ci sono 100 macchine ed una va in errore, c'è un punto centralizzato dove ti arrivano tutte le macchine e tutti gli errori.
Tutto basato su text. Le sfide di questo log aggregation sono che dobbiamo gestire diverse fonti di dati differenti, l'altra cosa è che abbiamo una grossa quantità di dati e se viene fatto su cloud costa.
Un esempio è \textbf{sentry}:
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Picture43.png}
\end{figure}
\noindent Una volta inizializzata il logging avviene automaticamente. Avremo quindi un debugging di qualità ed intuitivo. 
Ecco un esempio:
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Picture44.png}
\end{figure}
\begin{center}
    Unendo il tutto a sentry avremo una pagina web che ci dirà tutti i problemi.
    $\downarrow$
\end{center}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Picture45.png}
\end{figure}

\newpage
\section{Python Packaging}
Essendo pyton un interprete ci sono diversi problemi nell'import ricorrenti o ricorsivi, il discorso delle doppie dipendenze può portare ad errori. Stesso discorso con gli import relativi, per ovviare a questi problemi si indica, all'avvio del programma, la cartella di partenza.
\newline
L'ultima volta eravamo rimasti qua:
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{Picture46.png}
\end{figure}
\noindent Per strutturare tutto questo in un pacchetto il team di python ha deciso di mettere tutti i sorgenti dentro la cartella \textbf{src}, dobbiamo creare un pacchetto con un nome, in questo caso si chiama docker\_tests, e dentro una cartella src. 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{Picture47.png}
\end{figure}
\begin{center}
    $\Downarrow$
\end{center}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{Picture48.png}
\end{figure}
\noindent Fatto questo dobbiamo scrivere i test ed aggiornare il .gitignore.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\textwidth]{Picture49.png}
\end{figure}
\noindent Aggiorniamo un minimo il dockerfile:
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Picture50.png}
\end{figure}
\noindent Creaiamo ora il pacchetto:
\begin{itemize}
    \item è un modo per organizzare il nostro progetto in modo che le persone possano andare a prenderlo e scaricarlo senza vedere com'è strutturato tutto il progetto.
    \item è utile creare un pacchetto finale del nostro progetto.
\end{itemize}
Ma come si fa a creare un pacchetto python?
\begin{itemize}
    \item Il primo modo è quello di creare un file chiamato setup.py e qui definiamo lato codice "creami il pacchetto"
    \item Un altro modo più veloce e moderno è tramite software esterni come toml:
    \begin{figure}[H]
        \centering
        \includegraphics[width=1\textwidth]{Picture51.png}
    \end{figure}
\end{itemize}
\textbf{Poetry} è uno strumento moderno per la gestione delle dipendenze e la creazione di pacchetti Python. Rispetto ai metodi tradizionali (\texttt{setup.py}, \texttt{requirements.txt}), Poetry offre:

\begin{itemize}
    \item Gestione automatica delle dipendenze (inclusi sub-dependencies)
    \item Ambiente virtuale isolato per ogni progetto
    \item Comandi semplici per build, pubblicazione e gestione versioni
    \item Un unico file di configurazione: \texttt{pyproject.toml}
\end{itemize}

\textbf{Installazione:}
\begin{lstlisting}
pip install poetry
\end{lstlisting}

\textbf{Creazione di un nuovo progetto:}
\begin{lstlisting}
poetry new nome_progetto
\end{lstlisting}

\textbf{Aggiunta di una dipendenza:}
\begin{lstlisting}
poetry add numpy
\end{lstlisting}

\textbf{Attivazione dell'ambiente virtuale:}
\begin{lstlisting}
poetry shell
\end{lstlisting}

\textbf{Build e pubblicazione:}
\begin{lstlisting}
poetry build
poetry publish
\end{lstlisting}

\noindent Poetry semplifica la gestione dei progetti Python, rendendo più facile mantenere un ambiente riproducibile e pronto per la distribuzione.
Fatta la gestione delle dipendenze risulterà una cosa simile:
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Picture55.png}
\end{figure}
\noindent Il makefile aggiornato sarà così:
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Picture56.png}
\end{figure}
\noindent A questo punto facciamo build:
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Picture57.png}
\end{figure}
\noindent Ora si può installare con \texttt{pip install mlops-0.q.0-oy3-none-any-whl}:
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{Picture58.png}
\end{figure}
\noindent Una volta creato il toml facendo \texttt{pip install.} lui farà tutto.
Questi sono degli artifact, oggetti che vengono prodotti alla fine dell'operazione di build e che possiamo rendere disponibili agli altri utenti.
\newline 
Ecco un esempio di artifact:
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Picture59.png}
\end{figure}
\begin{itemize}
    \item checkout
    \item scarica la repository 
    \item setup dei file 
    \item build delle dipendenze (make install)
    \item operazioni di build 
    \item carichiamo il python package con tutto quello che c'è nella cartella dist
\end{itemize}
Ecco un esempio di deploy di modello addestrato:
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Picture60.png}
\end{figure}
\noindent Attenzione allo spazio degli artifatti, le repository pubbliche non hanno un limite, quelle private hanno 500MB. 

\section{AutoML}
L'AutoML (Automated Machine Learning) è un insieme di tecniche e strumenti che automatizzano le fasi principali del ciclo di vita di un progetto di machine learning, come la selezione del modello, la scelta e l'ottimizzazione degli iperparametri, la preparazione dei dati e la valutazione delle performance. L'obiettivo è rendere accessibile il machine learning anche a chi non è esperto, riducendo il tempo e la complessità necessari per sviluppare modelli efficaci.

\subsection{Caratteristiche principali di AutoML}
\begin{itemize}
    \item \textbf{Preprocessing automatico dei dati}: gestione di dati mancanti, normalizzazione, encoding delle variabili categoriche.
    \item \textbf{Selezione automatica dei modelli}: esplorazione di diversi algoritmi (es. alberi decisionali, reti neurali, SVM) per trovare quello più adatto al problema.
    \item \textbf{Ottimizzazione degli iperparametri}: ricerca automatica dei parametri migliori tramite tecniche come grid search, random search o ottimizzazione bayesiana.
    \item \textbf{Valutazione e selezione}: confronto tra modelli tramite metriche di performance e selezione automatica del migliore.
    \item \textbf{Deployment facilitato}: alcuni strumenti AutoML permettono di esportare direttamente il modello pronto per la produzione.
\end{itemize}

\subsection{Vantaggi e limiti}
\textbf{Vantaggi:}
\begin{itemize}
    \item Riduce la necessità di competenze avanzate di ML.
    \item Accelera il processo di sviluppo.
    \item Permette di testare rapidamente molte soluzioni.
\end{itemize}

\textbf{Limiti:}
\begin{itemize}
    \item Meno controllo sui dettagli del modello.
    \item Può essere computazionalmente costoso.
    \item Non sostituisce l'esperienza umana nell'interpretazione dei risultati e nella gestione dei dati.
\end{itemize}

\subsection{Esempi di strumenti AutoML}
\begin{itemize}
    \item \textbf{Auto-sklearn}: estensione di scikit-learn per la selezione automatica di modelli e iperparametri.
    \item \textbf{TPOT}: utilizza algoritmi genetici per ottimizzare pipeline di machine learning.
    \item \textbf{H2O AutoML}: piattaforma open source per la creazione automatica di modelli.
    \item \textbf{Google Cloud AutoML}, \textbf{Azure AutoML}, \textbf{Amazon SageMaker Autopilot}: soluzioni cloud che offrono pipeline AutoML complete.
\end{itemize}

\subsection{Esempio pratico con Auto-sklearn}
\begin{lstlisting}[language=Python, basicstyle=\ttfamily\footnotesize, breaklines=true, frame=single]
import autosklearn.classification
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Carica dati di esempio
X, y = load_digits(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Crea e addestra AutoML classifier
automl = autosklearn.classification.AutoSklearnClassifier(time_left_for_this_task=60)
automl.fit(X_train, y_train)

# Valuta il modello
y_pred = automl.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
\end{lstlisting}

\noindent In questo esempio, Auto-sklearn seleziona e ottimizza automaticamente il modello migliore per il dataset delle cifre scritte a mano, mostrando la semplicità d'uso di uno strumento AutoML.

\section{Tool utili}
Partiamo da Gradio, interfaccia web che ci permette di gestire input ed output di reti neurali, ha un sacco di strumenti ed accessori per la modifica del progetto. 
Si importa cosi:
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Picture61.png}
\end{figure}
\begin{center}
    $\Downarrow$
\end{center}
Definiamo una funzione, in questo caso un'interfaccia, input e output testuali ed infine facciamo launch. 
Un'implementazione con YOLO è:
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Picture62.png}
\end{figure}

\section{Progetto d'esame}
Fatto il progetto dobbiamo inviare il link github (scrivendoci un minimo di documentazione), devono esserci le github action e ci deve essere un minimo di storia (viene valutato l'utilizzo degli strumenti), bisogna usare il linter e unit tes.
Il progetto sarà valutato sui concetti visti a lezione di DevOps:
\begin{itemize}
    \item Python DevOps (git, lin, unit tests, artifacts);
    \item Dockerizzazione; 
    \item Training e testing di un piccolo modello;
    \item Github actions per CI/CD;
\end{itemize}
Come sorgente di dataset possimao scegliere quello che vogliamo, se usiamo dataset di immagini scegliamo tra le mille e le trentamila, non di più.
\newline
Il corso è superato sulla base di un punteggio assegnato al progetto con un minimo di 6.0 punti, con possibili punti bonus extra:
Il punteggio è così diviso:
\begin{itemize}
    \item Gt repository (pubblico e package python): fino a 2 punti
    \item EDA su un dataset a scelta (consiglio piccolo): fino a 2 punti
    \item Dockerizzazione della soluzione, compreso addestramento: fino a 4 punti 
    \item CI/CD (github actions): unit, linter, docker build, artifact release: fino a 2 punti
    \item Opzionale:
    \begin{itemize}
        \item docler image push su docker hub (pubblico): fino a 1 punto 
        \item interfaccia interattiva con Gradio correttamente dockerizzata: fino a 1 punto
    \end{itemize}
\end{itemize}
Nota importante, del progetto verrà valutata in modo ppreciso l'organizzazione ed il funzionamento, le performance conteranno poco viste le macchine limitate.
\end{document}